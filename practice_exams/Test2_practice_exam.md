### 1. A Generative AI Engineer needs to implement a safety guardrail for a chatbot that handles medical inquiries. The chatbot must avoid giving medical advice but still provide relevant information. What guardrail technique is most appropriate?

- Apply a classifier to flag risky outputs post-generation.
- Train the LLM on a dataset with neutral language.
- ✅ **Use metaprompts instructing the chatbot to defer medical advice and only provide general information.**
- Reduce the model temperature to limit randomness.

Metaprompts provide a proactive guardrail, ensuring the chatbot complies with safety and ethical guidelines.

---

### 2. A team is building a RAG system for answering technical queries. Despite having a robust retriever, the system’s responses lack specificity. How can they improve the quality of answers?

- Use a smaller LLM to focus on brevity.
- ✅ **Augment the retriever with a context-aware summarizer to provide more precise and relevant answers.**
- Fine-tune the retriever for classification tasks.
- Increase the size of the retriever index.

Combining retrieval with context-aware summarization ensures that responses are specific and relevant, improving the overall quality of answers.

---

### 3. A Generative AI Engineer is tasked with building a guardrail for an LLM used in customer feedback analysis. The system must block responses containing offensive language. What implementation is most effective?

- Fine-tune the LLM on a dataset with neutral language.
- Reduce the model’s temperature to minimize random outputs.
- Train a classifier to flag offensive responses after generation.
- ✅ **Integrate content filtering to detect and block offensive language in outputs.**

Content filtering ensures offensive language is blocked in real-time, maintaining user trust and compliance with ethical standards.

---

### 4. A Generative AI Engineer just deployed an LLM application at a digital marketing company that assists with answering customer service inquiries. Which metric should they monitor to evaluate the application’s production performance?

- Final perplexity scores for the training of the model.
- ✅ **Number of customer inquiries processed per unit of time.**
- Energy usage per query.
- HuggingFace Leaderboard values for the base LLM.

Throughput monitoring is critical in production environments to measure how effectively the application handles customer inquiries.

---

### 5. A Generative AI Engineer must clean a dataset of scanned financial documents containing extraneous information like watermarks and logos. What preprocessing step is most effective?

- Retain all data to preserve document authenticity.
- Apply a summarization model to condense extracted content.
- Remove sections with missing text.
- ✅ **Use image preprocessing techniques to filter out watermarks and logos before text extraction.**

Image preprocessing ensures cleaner text extraction, improving the quality of downstream tasks.

---

### 6. A Generative AI Engineer is developing a RAG application to answer questions about financial regulations. The retrieved content often contains irrelevant sections. What approach can improve the retrieval process?

- Rely on a rule-based filtering system.
- Increase the retrieval batch size to include more documents.
- Use a summarization model to reduce irrelevant content.
- ✅ **Implement an embedding model fine-tuned on financial text to improve semantic relevance.**

Fine-tuning an embedding model on domain-specific text improves retrieval relevance by aligning with the context of financial regulations.

---

### 7. A Generative AI Engineer is tasked with building a pipeline for retrieving technical documents and answering user queries. Which strategy best ensures accurate responses?

- Rely on generic prompts that do not include document-specific details.
- Use only pre-trained embeddings for document matching.
- ✅ **Combine document retrieval with context-based prompt augmentation to include relevant details in user queries.**
- Focus on using the smallest LLM available.

Combining document retrieval with context augmentation ensures accurate and context-aware responses tailored to user queries.

---

### 8. A Generative AI Engineer must evaluate the safety of responses generated by a medical chatbot. What qualitative metric is most critical?

- Stylistic tone for better engagement.
- Response speed to improve user experience.
- ✅ **Factual accuracy to ensure medically correct responses.**
- Length of the response for user readability.

Ensuring factual accuracy is critical for generating safe and reliable responses in medical applications.

---

### 9. A Generative AI Engineer is tasked with filtering irrelevant sections from product manuals used in a RAG system. The manuals include marketing descriptions, disclaimers, and technical specifications. Which sections should be retained?

- Retain all sections for completeness.
- ✅ **Retain only the technical specifications relevant to user queries.**
- Include disclaimers and technical specifications.
- Remove all sections and rely solely on pre-trained model knowledge.

Filtering irrelevant sections like marketing descriptions ensures the RAG system retrieves precise, contextually relevant information.

---

### 10. A Generative AI Engineer must clean a dataset of news articles for a RAG system. The articles include unrelated advertisements and redundant metadata. What preprocessing step is necessary?

- Merge all articles into a single file for consistency.
- ✅ **Remove advertisements and metadata to retain only the main article content.**
- Retain all parts of the dataset to preserve the original structure.
- Summarize each article into a shorter format.

Removing irrelevant content like advertisements and metadata ensures the dataset is clean, improving retrieval and query accuracy.

---

### 11. A Generative AI Engineer is designing a prompt for generating JSON-based responses from user queries about inventory details. Each response should include fields for item name, stock level, and location. Which prompt will achieve this?

- Include details about the stock, but do not enforce a format.
- Summarize the inventory details based on the query.
- ✅ **Generate a JSON response including item_name, stock_level, and location based on the query.**
- Provide information about the inventory.

A well-structured prompt ensures responses are formatted consistently in JSON and include all required fields.

---

### 12. A large e-commerce platform needs a product recommendation engine that generates personalized suggestions using user purchase histories. Which pipeline design best fits this use case?

- Generate recommendations based on randomly sampled products.
- Implement a purely generative approach without historical data.
- ✅ **Use embeddings for purchase history and generate recommendations based on similarity scores.**
- Use rule-based filters for predefined product categories.

Embedding-based pipelines are ideal for personalized recommendations as they account for user preferences and purchase histories.

---

### 13. A Generative AI Engineer must prioritize high-quality source documents for training a financial RAG system. Which sources should they focus on?

- News articles summarizing financial events.
- User-submitted financial forum posts.
- User-written financial blogs.
- ✅ **Regulatory filings and official financial reports.**

Using authoritative sources like regulatory filings ensures accurate, trustworthy data for financial RAG applications.

---

### 14. A Generative AI Engineer must identify resources required to serve features for a RAG application deployed at scale. What resources are critical? (Choose two)

- Tokenization optimization tools.
- High-speed internet connections to minimize latency.
- Additional LLM fine-tuning capabilities.
- Batch processing tools for low-latency responses.
- ✅ **Sufficient compute power for embeddings and retrieval, storage for vector data, and monitoring tools for performance.**

Compute power, storage, monitoring, and high-speed networks ensure efficient, scalable RAG application serving.

---

### 15. A Generative AI Engineer is integrating a third-party dataset into a RAG application. Some documents in the dataset are flagged for containing harmful misinformation. What mitigation strategy should they use?

- Retain flagged documents but monitor their impact during inference.
- Allow flagged documents but train the model to ignore them.
- ✅ **Exclude flagged documents during preprocessing to prevent misinformation from entering the application.**
- Use the flagged documents but add disclaimers to outputs.

Excluding flagged documents ensures the application avoids misinformation, maintaining reliability and ethical standards.

---

### 16. A Generative AI Engineer must register an LLM in Unity Catalog using MLflow for better governance and reproducibility. What steps should they follow to register the model?

- Fine-tune the model in Unity Catalog before deployment.
- ✅ **Log the trained model to MLflow, add the model signature, and register it in Unity Catalog for centralized management.**
- Train the model and directly upload it to Unity Catalog.
- Define the input-output formats in Unity Catalog first, then register the model in MLflow.

Registering an LLM in Unity Catalog involves logging the model in MLflow, defining its input-output signature, and integrating it for governance and management.

---

### 17. A Generative AI Engineer is evaluating the performance of a RAG application using MLflow. What metrics should they track to assess retrieval effectiveness?

- Model perplexity during training.
- Token usage per query.
- ✅ **Precision, recall, and NDCG (Normalized Discounted Cumulative Gain).**
- BLEU and ROUGE scores for evaluating text generation.

Tracking precision, recall, and NDCG ensures a comprehensive evaluation of retrieval performance in a RAG application.

---

### 18. A Generative AI Engineer needs to serve a Retrieval-Augmented Generation (RAG) application that uses a foundation model API. What is the key consideration for cost optimization?

- Remove retrievers to simplify the pipeline.
- Use a larger model to improve accuracy.
- ✅ **Batch multiple user queries during inference to reduce API calls.**
- Enable real-time query logging for monitoring.

Batching queries reduces API call frequency, which helps optimize costs while maintaining system performance.

---

### 19. A Generative AI Engineer must implement masking techniques to handle personally identifiable information (PII) in a customer feedback system. What is the primary advantage of this approach?

- Masking reduces the response time for processing user data.
- ✅ **Masking anonymizes sensitive data, reducing privacy risks while preserving analytical value for feedback processing.**
- Masking prevents model overfitting during training.
- Masking encrypts the dataset to protect it during storage.

Masking protects sensitive data by anonymizing PII, ensuring privacy compliance and preserving the usefulness of customer feedback data.

---

### 20. A Generative AI Engineer is selecting an LLM for a RAG application. The application retrieves long documents, and the model must summarize key sections accurately. What factors should guide the LLM choice?

- Token limit and BLEU scores.
- Model size and training dataset diversity.
- Retrieval precision and recall.
- ✅ **Context length, summarization quality, and inference speed.**

Context length, summarization quality, and inference speed ensure the LLM is suitable for long-document summarization in a RAG application.

---

### 21. A Generative AI Engineer is building a system for summarizing research papers. The papers often contain complex diagrams and tables. What should they prioritize when designing prompts?

- Emphasize summarizing text content only while ignoring diagrams and tables.
- Include diagrams and tables directly in the prompt.
- Prioritize summarizing only abstract sections.
- ✅ **Convert diagrams to text descriptions for summarization.**

Prompting the LLM to focus on text ensures high-quality summaries without introducing noise from unsupported content like diagrams.

---

### 22. Why is it essential to evaluate an LLM‘s context length when integrating it into a document summarization pipeline?

- To ensure shorter summaries.
- ✅ **To ensure the model can process input documents without truncation, preserving key information.**
- To reduce token usage and inference cost.
- To improve model training efficiency.

Evaluating context length ensures the model can process the full document without losing critical details due to truncation.

---

### 23. A Generative AI Engineer must implement guardrails to prevent a generative model from leaking sensitive data in its responses. What is the most effective strategy?

- Apply a summarization model to shorten responses.
- ✅ **Use metaprompts to explicitly instruct the model not to include sensitive data.**
- Use post-processing filters to remove sensitive information.
- Lower the model temperature to reduce randomness.

Metaprompts ensure proactive control over the model’s output, preventing sensitive data leakage effectively.

---

### 24. A Generative AI Engineer is tasked with designing an assistant for a financial services company. The assistant must recommend personalized savings plans based on user income, expenses, and goals. What should the prompt include? (Choose two)

- Provide an example of a successful savings plan.
- Use a generic prompt like “Generate a savings plan.“
- Avoid specifying goals to encourage flexibility.
- ✅ **Specify input details like income, expenses, and goals, and request an actionable recommendation. Example: “Income: $5000, Expenses: $3000. Goal: Save for a vacation. Provide a savings recommendation.“**

Specifying inputs (income, expenses, goals) and including examples ensures the model generates personalized, actionable savings recommendations aligned with user needs.

---

### 25. A Generative AI Engineer must evaluate the retrieval performance of a chatbot. The system retrieves many irrelevant documents but also misses some relevant ones. Which metrics should they focus on? (Choose four)

- ✅ **F1 Score**
- ✅ **Precision**
- Sentiment Analysis
- ✅ **Cosine Similarity**
- ✅ **Recall**

Focusing on precision, recall, F1 score, and cosine similarity ensures a comprehensive evaluation of retrieval performance, addressing both relevance and completeness.

---

### 26. A Generative AI Engineer is tasked with querying a large vector database for a knowledge management system. How should they optimize the query process for performance and accuracy?

- Skip embedding optimization and rely on raw text retrieval.
- Reduce query complexity by using smaller embedding models.
- ✅ **Use nearest-neighbor search algorithms with appropriate distance metrics for accurate retrieval.**
- Perform brute-force search for maximum precision.

Nearest-neighbor search with optimized distance metrics ensures high-performance, accurate vector database querying.

---

### 27. A Generative AI Engineer is building a pipeline to deploy an LLM that generates personalized travel itineraries. The model needs to handle location data, user preferences, and budget constraints. What key components should the pipeline include?

- ✅ **An embedding model for location and user preference representation, a retriever for querying data, and an LLM for generating itineraries.**
- A rule-based system for fixed travel recommendations.
- A classification model to categorize user preferences.
- A summarization model to condense travel options.

Embedding models, retrievers, and LLMs combine to generate accurate and personalized travel plans based on diverse user inputs.

---

### 28. A Generative AI Engineer must create a pipeline to retrieve and summarize customer reviews. Some reviews contain offensive language. How should they handle preprocessing?

- ✅ **Implement filtering to detect and remove offensive language before summarization.**
- Retain offensive language for context.
- Summarize without filtering.
- Exclude all reviews containing more than 10 words.

Filtering offensive language ensures that summaries remain appropriate and align with user expectations.

---

### 29. A Generative AI Engineer is building a multi-stage reasoning system to assist users in legal contract reviews. The system should identify potential issues, propose revisions, and highlight key clauses. What tools are essential? (Choose two)

- A visualization tool to display highlighted clauses.
- ✅ **A retriever to fetch relevant legal references and precedents.**
- ✅ **An embedding model to semantically analyze contract clauses.**
- A summarization model to condense contract text.
- A classification model to label clauses by type.

Combining embeddings and retrievers ensures effective issue identification and revision proposals for legal contracts.

---

### 30. A marketing agency needs an AI tool to generate engaging headlines for advertisements. The headlines should target specific demographics like “young professionals“ or “retirees.“ What should the prompt include for best results? (Choose two)

- Avoid specifying tone to encourage creativity.
- ✅ **Specify the tone, target audience, and desired outcome. Example: “Write a headline targeting young professionals. Tone: professional and motivating. Outcome: highlight career growth opportunities.”**
- Use a generic prompt like “Write engaging headlines for ads.“
- ✅ **Provide a few-shot example of successful headlines.**

Combining clear instructions (tone, audience, goals) with examples ensures the AI generates consistent, high-quality headlines that resonate with the intended audience.

---

### 31. A Generative AI Engineer is tasked with preparing a large technical document for a RAG system. The document contains appendices, tables of contents, and redundant footnotes that are irrelevant to the task. What preprocessing step should the engineer take?

- Highlight irrelevant sections to flag them but keep them in the document.
- Merge all sections of the document into a single chunk.
- ✅ **Remove irrelevant sections like appendices and tables of contents during preprocessing.**
- Keep the entire document intact for completeness.

Preprocessing ensures the removal of irrelevant sections, enhancing the quality of document retrieval for RAG applications.

---

### 32. A Generative AI Engineer is designing a pyfunc model that includes pre- and post-processing for sentiment analysis. What components should the engineer include?

- Use the model output directly without post-processing.
- Pass raw text to the model without preprocessing.
- ✅ **Preprocess input text (e.g., clean text, tokenize), call the model for sentiment prediction, and post-process the output into user-friendly labels.**
- Skip the model step and rely only on preprocessing.

Preprocessing, model inference, and post-processing together ensure high-quality sentiment analysis with interpretable outputs.

---

### 33. A Generative AI Engineer has developed a RAG application designed to answer questions about a series of fantasy novels discussed on the author’s web forum. The novels have been chunked and embedded into a vector store containing metadata like page numbers, chapter titles, and book names. User queries retrieve relevant chunks, which are then processed by an LLM for generating responses. Initially, the engineer used intuition to select chunking strategies and configurations but now wants a systematic approach to optimize these settings. Which TWO approaches should the engineer adopt to refine the chunking strategy and parameters? (Choose two.)

- Change embedding models and compare performance.
- Add a classifier for user queries that predicts which book will best contain the answer. Use this to filter retrieval.
- ✅ **Choose an appropriate evaluation metric (e.g., recall or NDCG) and test chunking strategies such as splitting chunks by paragraphs or chapters. Select the strategy with the highest performance metric.**
- ✅ **Create an LLM-as-a-judge metric to assess how well previous questions are answered by specific chunks. Use this metric to optimize chunking parameters.**
- Pass known questions and best answers to an LLM and instruct it to identify optimal token counts. Use a summary statistic (mean, median, etc.) to determine chunk sizes.

A systematic evaluation using metrics like recall and NDCG, combined with LLM-as-a-judge metrics, ensures the best chunking strategy is selected for optimal performance.

---

### 34. A Generative AI Engineer is deploying a LLM-based RAG application for financial document analysis. The application must minimize costs while processing lengthy documents. Which strategies are most effective? (Choose three)

- ✅ **Optimize token usage by batching multiple queries into a single request.**
- ✅ **Enable inference logging to monitor query costs.**
- ✅ **Reduce the retrieval set size to minimize processing overhead.**
- Increase context length to maximize token usage for each query.
- Use a smaller LLM with high context length support to handle long inputs efficiently.

Combining smaller models, batching, and inference logging ensures cost-effective performance for processing lengthy financial documents.

---

### 35. A Generative AI Engineer is designing a pyfunc model to process financial data with pre- and post-processing steps. The model must clean the input, perform predictions, and format results for users. What workflow ensures this functionality?

- Apply predictions first and handle pre- and post-processing in a separate pipeline.
- Directly input raw data into the model and rely on post-processing for cleanup.
- Skip post-processing and return raw model outputs to users.
- ✅ **Preprocess financial data (e.g., normalize values), apply the prediction model, and post-process outputs into user-friendly formats like JSON.**

Including preprocessing, model inference, and post-processing steps ensures the model processes data effectively and outputs user-ready results.

---

### 36. A Generative AI Engineer must deploy a simple chain using LangChain to enable document retrieval and summarization. Why is LangChain well-suited for this task?

- It reduces computational costs by optimizing embeddings.
- ✅ **It provides modular tools for connecting retrievers, embedding models, and language models into a unified pipeline.**
- It enhances security for sensitive data by encrypting model outputs.
- It automatically fine-tunes models for retrieval-based tasks.

LangChain’s modular tools enable developers to build complete retrieval and summarization pipelines efficiently, making it an excellent choice for such tasks.

---

### 37. A Generative AI Engineer must control access to a deployed model serving endpoint. What techniques can they use to ensure secure access?

- Restrict access to specific IP addresses but skip authentication.
- Deploy the model on a private network without additional access controls.
- Use a public endpoint with a shared access key for all users.
- ✅ **Implement API key authentication and role-based access controls (RBAC).**

API keys and RBAC provide robust security, ensuring only authorized users can access the deployed model endpoint.

---

### 38. A Generative AI Engineer is integrating a conversational AI model into a web application for restaurant reservations. The system must return concise responses to user queries. How should they optimize the prompt?

- Allow the user to choose response length dynamically.
- Rely on the LLM’s temperature setting alone to reduce response length.
- Use default prompts without modifications.
- ✅ **Specify in the prompt that the responses should be brief, such as “Provide a short confirmation message for the reservation.”**

Including clear instructions about response length in the prompt ensures the system generates concise and effective messages.

---

### 39. A Generative AI Engineer must design a prompt to extract meeting summaries. Each summary should include action items, participants, and decisions made. What is the best prompt design?

- ✅ **Extract a meeting summary including action items, participants, and decisions made, based on the transcript.**
- Summarize the meeting transcript into a concise report.
- Highlight the key points of the meeting in a bullet-point format.
- Provide an overview of the meeting discussion.

Explicit prompts defining required fields ensure the model generates structured, actionable meeting summaries.

---

### 40. A Generative AI Engineer must evaluate the cost-effectiveness of two LLMs for generating responses in a production chatbot. What metrics should they compare? (Choose two)

- Model size in terms of parameters.
- ✅ **Throughput under high-load scenarios.**
- BLEU and perplexity for output quality.
- ✅ **Cost per query and latency.**
- Token limit and input size flexibility.

Comparing cost per query, latency, and throughput ensures a comprehensive evaluation of the cost-effectiveness of LLMs.

---

### 41. A Generative AI Engineer must deploy an endpoint for a RAG application. The endpoint must handle real-time queries and integrate retrieval, embeddings, and LLMs. What sequence of steps should the engineer follow? (Choose two)

- Use retrieval without vector search for simpler implementation.
- Deploy the endpoint before configuring embeddings.
- ✅ **Train embeddings → Create vector store → Integrate retrieval → Deploy the endpoint.**
- Skip training embeddings and directly create a vector store.
- Fine-tune the LLM before deploying the endpoint.

Proper sequencing ensures an efficient and fully integrated endpoint for real-time RAG applications.

---

### 42. A Generative AI Engineer is tasked with designing a RAG system for retrieving technical documentation and generating user-friendly responses. Users report that some responses include irrelevant details. What should the engineer prioritize to improve the system?

- ✅ **Refine the retrieval strategy by filtering results based on relevance scores.**
- Use a summarization model to simplify retrieval results.
- Fine-tune the model on technical query data.
- Increase the model temperature to reduce redundancy.

Refining the retrieval strategy ensures that the system retrieves only the most relevant content, reducing irrelevant details in responses.

---

### 43. A Generative AI Engineer is evaluating retrieval performance for a RAG application using scientific documents. Which metrics are best suited for this evaluation?

- Latency to measure speed of retrieval.
- ✅ **Precision and recall to measure relevance and coverage of retrieved results.**
- BLEU to measure textual similarity.
- Token usage to evaluate computational costs.

Precision and recall are key metrics for evaluating the relevance and completeness of retrieval performance in a RAG application.

---

### 44. A Generative AI Engineer must choose an LLM for a customer support chatbot. The chatbot should balance performance and cost while handling common queries effectively. Which metric should guide the selection?

- BLEU scores for text generation.
- Model size in terms of parameters.
- ✅ **Latency and inference cost per query.**
- Token limit of the LLM.

Latency and inference cost metrics ensure that the LLM meets the performance and cost requirements of a customer support chatbot.

---

### 45. A Generative AI Engineer is designing an LLM-powered tool for summarizing legal contracts. The model often generates hallucinated clauses. What solution should they implement?

- Increase the context length to capture more document content.
- Use a classification model to validate generated summaries.
- ✅ **Write a metaprompt instructing the model to generate summaries only based on the provided text.**
- Apply a summarization model pre-trained on legal datasets.

Metaprompts guide the LLM to focus strictly on source content, reducing the likelihood of hallucinations in contract summaries.

---

### 46. A Generative AI Engineer must evaluate a dataset for legal risks before using it to train a language model. What key factor should they consider?

- The size of the dataset to ensure it meets training requirements.
- The model architecture being used for training.
- ✅ **Licensing terms that dictate permissible usage and any restrictions on data applications.**
- The presence of sensitive information in the dataset.

Reviewing licensing terms ensures the dataset is used in accordance with its legal permissions, avoiding potential disputes or violations.

---

### 47. An AI model generates recommendations for customer queries but frequently suggests irrelevant products. What adjustment should an engineer prioritize?

- Increase the model’s temperature during inference.
- Reduce response length to avoid irrelevant recommendations.
- ✅ **Improve embeddings to better capture query-product semantic similarity.**
- Add more parameters to the model.

High-quality embeddings improve the semantic alignment between queries and products, addressing issues of irrelevance in recommendations.

---

### 48. A Generative AI Engineer must deploy a LangChain-based application for retrieving legal documents and generating summaries. What is the primary role of LangChain in this system?

- Managing cloud resources for scalable deployment.
- ✅ **Orchestrating the workflow by connecting retrievers, LLMs, and preprocessing steps.**
- Automating the fine-tuning of LLMs.
- Visualizing data flows in the application.

LangChain connects retrievers, preprocessors, and LLMs in a unified pipeline, making it ideal for building modular retrieval and summarization workflows.

---

### 49. A Generative AI Engineer is tasked with coding a LangChain application to automate querying a product catalog. The system must retrieve product details based on natural language queries. What components are necessary for the chain? (Choose two)

- ✅ **A retriever to fetch product details from the catalog.**
- A summarization model to condense results.
- ✅ **A prompt template to structure user queries for the LLM.**
- A rule-based classifier to categorize queries.
- A pre-trained sentiment analysis model.

Combining retrievers and prompt templates ensures accurate and efficient responses to natural language queries.

---

### 50. LangChain is commonly used for integrating LLMs with retrieval systems. Which key feature of LangChain makes it ideal for this task?

- Its primary focus on model fine-tuning.
- ✅ **Its ability to create structured workflows that combine document retrieval and generative capabilities.**
- Its use of open-source pre-trained embeddings.
- Its built-in visualization tools for output comparison.

LangChain’s ability to orchestrate workflows between retrieval and generation tasks makes it an excellent choice for Generative AI applications.

---

### 51. A Generative AI Engineer is tasked with developing an AI tool for generating marketing content. The tool must create product descriptions optimized for e-commerce platforms. What should the prompt include? (Choose two)

- Use a generic prompt like “Write a product description.“
- Avoid specifying tone to encourage creativity.
- ✅ **Provide few-shot examples of successful descriptions.**
- ✅ **Specify product details, tone, and length. Example: “Create a description for a smartwatch. Tone: professional. Length: 100 words.”**

Combining clear instructions (tone, length, details) with examples ensures product descriptions are tailored to the platform and resonate with the target audience.

---

### 52. A Generative AI Engineer must protect a generative application from adversarial user inputs designed to crash the system. What is the best defensive strategy?

- Monitor user inputs and log suspicious activity.
- Lower the model’s response complexity to limit crash risks.
- ✅ **Implement input validation rules to check for unacceptable patterns and formats.**
- Train the model on a dataset designed to handle adversarial inputs.

Input validation rules ensure the application can handle adversarial inputs securely, protecting system stability.

---

### 53. A Generative AI Engineer must build a QA system for a retail company. The system should retrieve answers about product inventory and availability. What tools should the pipeline include?

- ✅ **An embedding model to represent inventory data semantically.**
- A classification model to categorize products.
- A summarization model to condense product descriptions.
- ✅ **A retriever to fetch inventory data and an LLM to generate responses.**

A combination of retrievers and LLMs ensures accurate answers to inventory-related queries, meeting user requirements.

---

### 54. A Generative AI Engineer is integrating an LLM into a sales analytics platform. The LLM must suggest actions based on customer purchase patterns. Which feature is critical for success?

- Using a default pre-trained LLM without modification.
- Relying solely on embeddings without inference capabilities.
- Choosing the largest available LLM for broader capabilities.
- ✅ **Fine-tuning on purchase history data for better predictions.**

Fine-tuning on domain-specific data ensures the LLM generates actionable insights aligned with customer purchase patterns.

---

### 55. A Generative AI Engineer needs to generate embeddings for a question-answer retrieval system. User queries are short, while documents are long. Which embedding model strategy is most effective?

- ✅ **Use a dual encoder with separate embeddings for queries and documents.**
- Use a classification model instead of an embedding model.
- Rely on pre-trained embeddings without customization.
- Use a single encoder for both queries and documents.

A dual encoder ensures that both short queries and long documents are effectively represented, improving retrieval performance.

---

### 56. Which indicator should be considered to evaluate the safety of the LLM outputs when qualitatively assessing LLM responses for a translation use case?

- ✅ **The accuracy and relevance of the responses**
- The ability to generate responses in code
- The similarity to the previous language
- The latency of the response and the length of text generated

Accurate and relevant outputs ensure safe and effective responses in translation use cases by aligning with the user’s needs and context.

---

### 57. A Generative AI Engineer must implement guardrails for a chatbot handling sensitive customer data. What is the most critical step?

- ✅ **Filter responses to detect and block any potential private data leakage.**
- Train the chatbot on smaller datasets.
- Enable unrestricted response generation.
- Shorten the responses to avoid excessive detail.

Filtering responses for private data ensures compliance with privacy standards and prevents sensitive data exposure.

---

### 58. A Generative AI Engineer is tasked with fine-tuning a model to identify key topics in technical documents. The dataset contains unnecessary metadata such as timestamps and author names. How should the engineer handle preprocessing?

- Process the dataset without filtering.
- Retain metadata to provide context.
- ✅ **Remove all irrelevant metadata during preprocessing.**
- Exclude all numerical data, including timestamps.

Removing metadata ensures the focus remains on relevant document content, enhancing topic identification accuracy.

---

### 59. A Generative AI Engineer must deploy a PyFunc model that preprocesses input text by cleaning noisy characters and post-processes the output by formatting the result into a JSON structure. Where should the preprocessing and post-processing steps be implemented?

- At the deployment server level via a custom API wrapper.
- As standalone Python scripts outside the PyFunc model.
- ✅ **Inside the PyFunc wrapper’s predict function, as part of the model logic.**
- Using a data preprocessing library such as Pandas.

PyFunc models allow custom pre- and post-processing logic to be encapsulated within the predict function, ensuring streamlined and integrated workflows during inference.

---

### 60. A Generative AI Engineer must implement guardrails for a chatbot handling legal inquiries to prevent generating inaccurate legal advice. What technique is most effective? (Choose two)

- Lower the model temperature to limit output variability.
- ✅ **Use metaprompts to instruct the chatbot to limit responses to general information, avoiding legal advice.**
- Block user queries containing legal terms.
- Train the chatbot on legal datasets to improve accuracy.
- ✅ **Add a disclaimer in all chatbot outputs clarifying its limitations.**

The correct answers are:
1) Use metaprompts to instruct the chatbot to limit responses to general information, avoiding legal advice.
2) Add a disclaimer in all chatbot outputs clarifying its limitations.
Why these are correct?

Metaprompts → Define system behavior (e.g., "Only provide general legal information, do not give legal advice"). This is a core technique for guardrails.
Disclaimers → Ensure users understand the chatbot's limitations and reduce liability risk.
Why not the others?

Lower the model temperature → Reduces randomness but does not prevent giving incorrect legal advice.
Block user queries containing legal terms → Too restrictive and harms user experience.
Train the chatbot on legal datasets → Improves knowledge but does not stop it from accidentally giving legal advice.
