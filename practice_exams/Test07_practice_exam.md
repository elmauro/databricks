### 1. You need to build a scalable, low-latency text generation model for marketing content that maintains coherence, relevance, and controlled length. It should support fine-tuning with domain-specific data. Which architecture is best suited for this task?

- Transformer Encoder-Only Model
- BERT (Bidirectional Encoder Representations from Transformers)
- ✅ **GPT-3 (Generative Pre-trained Transformer)**
- LSTM (Long Short-Term Memory) Network

C. GPT-3 (Generative Pre-trained Transformer)
GPT-3 and similar decoder-only transformer architectures are well-suited for generative tasks like text generation.

---

### 2. When deploying a basic Retrieval-Augmented Generation (RAG) application in Databricks, which of the following sequences correctly represents the model registration, packaging, and deployment steps?

- Register the language model with MLflow → Develop the retrieval function → Package the application → Deploy the endpoint
- Develop the retrieval function → Package the application → Train the language model → Deploy the endpoint
- Train the language model → Package the application → Register the model with MLflow → Deploy the endpoint → Develop the retrieval function
- ✅ **Develop the retrieval function → Train the language model → Register the model with MLflow → Package the application → Deploy the endpoint**

D. Develop the retrieval function → Train the language model → Register the model with MLflow → Package the application → Deploy the endpoint
This sequence correctly outlines the basic steps for deploying a RAG application in Databricks.

---

### 3. You are tasked with evaluating a Retrieval-Augmented Generation (RAG) model for a customer support chatbot using MLflow. The RAG model retrieves relevant documents and generates responses. After training several models, you want to log and compare their performance. Which metrics should you log in MLflow to assess the model’s ability to generate useful and accurate responses?

- Precision, Recall, F1-Score
- Latency (time to generate a response) and memory usage
- The total number of training epochs and learning rate
- ✅ **BLEU (Bilingual Evaluation Understudy) score and ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score**

D. BLEU and ROUGE are standard metrics used to evaluate the quality of generated text.

---

### 4. You are developing a pipeline to extract large volumes of streaming data from an IoT device platform for analysis using a generative AI model. The data includes structured logs and unstructured images. Which of the following tools or methods would be the most appropriate for extracting and processing the data in real-time?

- Apache Kafka
- Databricks SQL
- ✅ **Databricks Auto Loader with File Notification Mode**
- Apache Flume

C. Auto Loader with File Notification Mode efficiently ingests new files as they arrive.

---

### 5. When assessing the quality of responses generated by a large language model (LLM) in a financial advisory application, which of the following is the most effective indicator of a high-quality response?

- The response provides multiple perspectives without clear guidance, ensuring neutrality.
- ✅ **The response includes relevant and up-to-date financial data that is accurate and actionable.**
- The response is generated quickly, indicating high model efficiency.
- The response uses highly technical jargon to demonstrate expertise, even if the user may not fully understand it.

B. Accurate, actionable, and current financial data indicates high-quality responses.

---

### 6. You are tasked with writing a large, chunked text dataset into Delta Lake tables within Unity Catalog. The data needs to be prepared efficiently for querying and analysis. Which of the following is the correct sequence of operations to write the chunked text data into a Delta Lake table?

- Combine chunks → Convert to DataFrame → Define Delta Table schema → Write to Delta Lake in Merge mode
- Convert to DataFrame → Combine chunks → Write to Delta Lake in Append mode
- ✅ **Combine chunks → Convert to DataFrame → Write to Delta Lake in Overwrite mode**
- Combine chunks → Create Delta Table schema → Write to Delta Lake in Append mode

C. Combine, convert to DataFrame, then overwrite for the initial complete write.

---

### 7. A manufacturing company wants to implement an AI-powered predictive maintenance system to predict machine failures using sensor data. The system should output a failure prediction for the next 30 days based on the input sensor data. Which description best translates this goal into inputs and outputs for an AI pipeline?

- “The input should be the machine operating conditions and sensor data, and the output should be a classification of the machine’s current state”
- “The input should be the historical machine usage data, and the output should be a list of repairs needed for each machine”
- ✅ **“The input should be the machine’s sensor data and operating history, and the output should be a probability score indicating the likelihood of machine failure within the next 30 days”**
- “The input should be a summary of sensor anomalies, and the output should be a timeline of potential future failures”

C. Probability score within 30 days directly supports predictive maintenance decisions.

---

### 8. You have trained a machine learning model in Databricks and now wish to make it available for deployment and version control by registering it in the Databricks Model Registry. What is the correct sequence of steps required to register the model?

- Train the model, save it as a .pkl file locally, and upload it to the registry.
- Save the model in a cloud storage bucket and manually configure the Model Registry to link to the bucket.
- ✅ **Train the model, log it to MLflow tracking, and use the MLflow UI or API to register it in the Model Registry.**
- Directly deploy the model without registering it, as registration is optional for deployment.

C. Train → log with MLflow → register in the Model Registry.

---

### 9. You work for a retail company seeking to optimize its product recommendation system using a generative AI model to provide personalized suggestions based on past interactions. The company has a large dataset of customer behavior, product features, and transaction history. As the AI engineer, how would you design the AI pipeline to meet these requirements and structure the inputs and outputs?

- ✅ **Input: Customer purchase and browsing history; Output: A list of recommended products ranked based on predicted user preference**
- Input: Customer profiles and transaction history; Output: A generative model that produces product descriptions from scratch
- Input: Transaction history and time series data; Output: Forecasted product demand for future time periods
- Input: User-provided real-time data such as clicks, reviews, and transaction history; Output: Pre-defined product categories that match user profile

A. Personalized ranked recommendations from historical interactions.

---

### 10. You are building a retrieval-augmented generation (RAG) application for a scientific research organization to assist researchers in summarizing articles. Only content from abstracts and body sections is important, while references, author details, and acknowledgments should be excluded. What is the best approach to filter out extraneous content and ensure the system focuses on high-quality summaries?

- Leave all sections intact, but train the model to prioritize content based on semantic relevance using topic modeling.
- ✅ **Write a rule-based system that explicitly detects and removes sections like “References”, “Acknowledgments”, and “Author Information” based on section headers.**
- Use a machine learning model to identify and remove irrelevant sections such as references, acknowledgments, and author details.
- Remove all sections of the document except for the abstract, as it provides a concise summary of the research.

B. Rule-based header filtering precisely removes predictable, extraneous sections.
