# üöÄ Resumen R√°pido - Certificaci√≥n Databricks GenAI Engineer Associate

## üìå Uso: Consulta R√°pida Antes del Examen

---

## 1Ô∏è‚É£ FUNDAMENTOS (25% del examen)

### Conceptos Clave
- **IA Generativa** = Crear contenido nuevo (texto, imagen, c√≥digo, audio, video)
- **LLMs** = Modelos grandes entrenados con billones de tokens
- **Pre-entrenamiento** = Conocimiento general | **Fine-tuning** = Especializaci√≥n

### LLMs Importantes
| Modelo | Tipo | Nota |
|--------|------|------|
| **DBRX** | Databricks OSS | Mixture-of-Experts |
| **GPT-4** | OpenAI Propietario | Mejor calidad general |
| **LLaMA 3** | Meta OSS | 8B, 70B params |
| **Dolly** | Databricks | Primer OSS empresarial |

### Stack Databricks
```
Unity Catalog ‚Üí Gobernanza centralizada, ACLs, Lineage, Auditor√≠a
Delta Lake ‚Üí ACID Transactions, Time Travel, Auto-Optimization
Vector Search ‚Üí B√∫squeda sem√°ntica, Auto-sync con Delta Tables
MLflow ‚Üí Tracking, Registry, Evaluation, AI Gateway
Model Serving ‚Üí Producci√≥n (APIs), Auto-scaling, Inference Tables
Mosaic AI ‚Üí Foundation Models, Agent Framework, AI Playground
Delta Live Tables ‚Üí ETL con Quality Expectations
Feature Serving ‚Üí Features en tiempo real
```

### Unity Catalog (Gobernanza)
- **ACLs por Nivel**: Data (SELECT, INSERT), AI/ML (EXECUTE, MODIFY), Infra (CREATE, MANAGE)
- **Lineage Autom√°tico**: Rastrea origen y uso de datos/modelos
- **Auditor√≠a**: Registra todos los accesos (qui√©n, cu√°ndo, qu√©)
- **Compliance**: GDPR, HIPAA, SOC2 autom√°tico

### Delta Lake (Storage)
- **ACID**: Atomicity, Consistency, Isolation, Durability
- **Time Travel**: `VERSION AS OF` o `TIMESTAMP AS OF`
- **Auto Optimize**: Compacta archivos, Z-Ordering, Data Skipping
- **Vacuum**: Limpia versiones antiguas
- **Para GenAI**: Guarda embeddings, inference logs, chunks

### Riesgos
- **Hallucination**: Inventar datos falsos
- **Bias**: Sesgos en datos/respuestas
- **Prompt Injection**: Manipular LLM
- **Privacy**: Exponer datos sensibles

---

## 2Ô∏è‚É£ DESARROLLO DE SOLUCIONES (30% del examen)

### Prompt Engineering

**T√©cnicas**:
- **Zero-shot**: Sin ejemplos
- **Few-shot**: Con ejemplos
- **Chain-of-Thought**: Pensar paso a paso
- **Prompt Chaining**: Encadenar prompts

**Componentes de Prompt**:
1. Instrucciones
2. Contexto
3. Input/Pregunta
4. Formato output

### RAG (Retrieval-Augmented Generation)

**Flujo**: Query ‚Üí Retrieve docs ‚Üí Augment prompt ‚Üí Generate

**Ventajas**: Conocimiento actualizado, reduce hallucinations

**Arquitectura**:
```
Documents ‚Üí Delta Live Tables ‚Üí Chunking ‚Üí Embeddings ‚Üí 
Vector Search ‚Üí Retrieval ‚Üí LLM ‚Üí Response
```

### Chunking

| Tipo | Descripci√≥n | Pros/Contras |
|------|-------------|--------------|
| **Fixed-size** | Tama√±o fijo | Simple pero puede cortar contexto |
| **Context-aware** | Por secci√≥n/p√°rrafo | Mejor contexto, m√°s complejo |
| **Overlap** | Superposici√≥n | Mantiene info entre chunks |

**Tip**: Usar overlap de 10-20% entre chunks

### Embeddings

- **Qu√© son**: Representaci√≥n vectorial de texto
- **Tip Cr√≠tico**: Usar el MISMO modelo para docs y queries
- **Modelos**: BGE-Large, E5, OpenAI Ada, Cohere Embed

### Vector Search (Databricks)

**Setup**:
1. Crear Vector Search Endpoint
2. Crear Model Serving Endpoint (embeddings)
3. Crear Vector Search Index (desde Delta Table)

**Auto-sync con Delta Tables** ‚úÖ

### Reranking

**Qu√© hace**: Reordena top-K resultados para mejorar precisi√≥n

**Cu√°ndo usar**: Cuando precisi√≥n es cr√≠tica

**Modelos**: bge-reranker, Cohere Rerank, FlashRank

### M√©tricas RAG

| M√©trica | Qu√© Mide |
|---------|----------|
| **Context Precision** | ¬øDocs retrieved son relevantes? |
| **Context Recall** | ¬øRecuperamos todos los docs relevantes? |
| **Faithfulness** | ¬øRespuesta fiel al contexto? |
| **Answer Relevancy** | ¬øResponde la pregunta? |
| **Answer Correctness** | ¬øEs factualmente correcta? |

---

## 3Ô∏è‚É£ DESARROLLO DE APLICACIONES (20% del examen)

### Compound AI Systems

**Definici√≥n**: Sistema con m√∫ltiples componentes interactuando

**vs Simple System**: Simple = 1 paso | Compound = m√∫ltiples pasos

**Ejemplo**: Query ‚Üí Classifier ‚Üí Tool 1 ‚Üí Tool 2 ‚Üí Aggregator ‚Üí Response

### Frameworks

| Framework | Mejor Para |
|-----------|------------|
| **LangChain** | Aplicaciones generales |
| **LlamaIndex** | RAG avanzado |
| **Haystack** | Document processing |
| **DSPy** | Optimizaci√≥n autom√°tica |

### Agents

**Qu√© son**: Sistemas que deciden din√°micamente qu√© hacer

**Componentes**:
1. **Task** (tarea)
2. **LLM** (cerebro)
3. **Tools** (herramientas externas)
4. **Memory & Planning**

**ReAct Pattern**:
```
Thought ‚Üí Act ‚Üí Observe ‚Üí Thought ‚Üí Act ‚Üí ...
```

**Multi-Agent**: Varios agents especializados colaborando

### Multi-Modal AI

**Tipos**:
- Text ‚Üí Image (DALL-E)
- Image ‚Üí Text (GPT-4 Vision)
- Text + Image ‚Üí Text (Claude Vision)

**Modelo Clave**: **CLIP** = embeddings text + image en mismo espacio

---

## 4Ô∏è‚É£ DESPLIEGUE Y MONITOREO (15% del examen)

### M√©todos de Deployment

| M√©todo | Latencia | Mejor Para |
|--------|----------|------------|
| **Batch** | Horas/d√≠as | Reportes, ETL |
| **Streaming** | Segundos | Eventos real-time |
| **Real-Time** | < 2 seg | Chatbots, APIs |
| **Edge** | Milisegundos | IoT, offline |

### MLflow

**Tracking**: Loguear experimentos, par√°metros, m√©tricas

**Registry (Unity Catalog)**:
- Versioning autom√°tico
- Aliases: `@champion`, `@challenger`, `@staging`
- Lineage completo
- ACLs

**Pyfunc**: Interfaz gen√©rica para deployment

### Databricks Model Serving

**Soporta**:
1. Custom Models (MLflow)
2. Foundation Models (DBRX, LLaMA)
3. External Models (OpenAI, Anthropic)

**Features**:
- Auto-scaling
- A/B testing (traffic splitting)
- Inference Tables (auto-logging)
- Scale-to-zero

### Monitoring

**Databricks Lakehouse Monitoring**:
- Profile metrics (estad√≠sticas)
- Drift metrics (cambios en datos)
- Custom metrics

**Qu√© monitorear**:
- Input data
- Vector DB data
- Human feedback
- Prompts/responses
- Latency, cost, quality

### MLOps vs LLMOps

**MLOps** = DataOps + DevOps + ModelOps

**LLMOps adiciona**:
- Text templates (prompts)
- Chains completas
- Vector databases
- Human feedback loop
- Cost management (pay-per-token)

---

## 5Ô∏è‚É£ EVALUACI√ìN Y GOBERNANZA (10% del examen)

### Issues Cr√≠ticos

| Issue | Mitigaci√≥n |
|-------|------------|
| **Prompt Injection** | Guardrails, Llama Guard |
| **Hallucinations** | RAG, fact-checking |
| **Bias** | Auditar datos, fairness metrics |
| **Privacy** | Unity Catalog ACLs, data anonymization |

### Guardrails

**Tipos**:
1. **Input**: Prevenir inputs maliciosos
2. **Output**: Filtrar respuestas problem√°ticas
3. **Contextual**: Guiar comportamiento LLM

**Llama Guard**: Clasificador de contenido unsafe (Meta)

### M√©tricas LLM

**Base**:
- **Loss**: Error de predicci√≥n
- **Perplexity**: Sorpresa del modelo (menor = mejor)
- **Toxicity**: Contenido da√±ino (menor = mejor)

**Task-Specific**:
- **BLEU**: Traducci√≥n
- **ROUGE**: Summarization

**Benchmarks**:
- MMLU (conocimiento general)
- HellaSwag (common sense)
- HumanEval (c√≥digo)

### LLM-as-a-Judge

**Cu√°ndo usar**: No hay m√©tricas autom√°ticas o ground truth

**C√≥mo**: Usar un LLM (GPT-4) para evaluar respuestas de otro LLM

### Evaluaci√≥n End-to-End

**M√©tricas**:
- **Costo**: Cost per request, GPU hours
- **Performance**: Latency (P50, P95, P99), throughput
- **Quality**: CSAT, accuracy, task completion rate
- **Custom**: Deflection rate, time saved

### Offline vs Online

**Offline**:
- Dataset curado
- Reproducible
- Pre-deployment

**Online**:
- Usuarios reales
- A/B testing
- Production monitoring

**Estrategia**: Offline ‚Üí Canary (10%) ‚Üí Full production

---

## üéØ F√ìRMULAS Y N√öMEROS CLAVE

### Context Window
- GPT-3.5: 4K tokens
- GPT-4: 8K, 32K, 128K
- Claude 3: 200K tokens

### Embeddings Dimensions
- BGE-Large: 1024
- OpenAI Ada: 1536
- E5: 768

### Latency Targets
- Chatbot: < 2 segundos
- Streaming: < 5 segundos
- Batch: Horas/d√≠as OK

### A/B Testing
- Empezar con 5-10% tr√°fico nuevo modelo
- Incrementar gradualmente si m√©tricas buenas

---

## üìã COMANDOS Y C√ìDIGO ESENCIALES

### Unity Catalog (ACLs)
```sql
-- Dar permisos
GRANT SELECT ON TABLE catalog.schema.table TO `group_name`;
GRANT EXECUTE ON MODEL catalog.schema.model TO `developers`;

-- Revocar permisos
REVOKE INSERT ON TABLE catalog.schema.sales FROM `interns`;

-- Ver permisos
SHOW GRANTS ON TABLE catalog.schema.customer_data;
```

### Delta Lake
```sql
-- Time Travel
SELECT * FROM catalog.schema.sales VERSION AS OF 100;
SELECT * FROM catalog.schema.sales TIMESTAMP AS OF '2024-01-15';

-- Optimizaci√≥n
OPTIMIZE catalog.schema.sales ZORDER BY (date, region);

-- Auto Optimize (activar)
ALTER TABLE catalog.schema.data SET TBLPROPERTIES (
  'delta.autoOptimize.optimizeWrite' = 'true',
  'delta.autoOptimize.autoCompact' = 'true'
);

-- Vacuum (limpiar versiones antiguas)
VACUUM catalog.schema.table RETAIN 168 HOURS;

-- Ver historial
DESCRIBE HISTORY catalog.schema.table;
```

### MLflow Registry
```python
# Registrar modelo
mlflow.langchain.log_model(
    model,
    "model",
    registered_model_name="catalog.schema.model_name"
)

# Cargar por alias
model = mlflow.langchain.load_model(
    "models:/catalog.schema.model_name@champion"
)
```

### Vector Search
```python
# Crear √≠ndice
vsc.create_delta_sync_index(
    endpoint_name="endpoint",
    index_name="catalog.schema.index",
    source_table_name="catalog.schema.docs",
    primary_key="id",
    embedding_source_column="text",
    embedding_model_endpoint_name="embed_endpoint"
)

# Buscar
results = index.similarity_search(
    query_text="mi query",
    num_results=5
)
```

### Model Serving
```python
# Crear endpoint
w.serving_endpoints.create(
    name="my-endpoint",
    config={
        "served_models": [{
            "model_name": "catalog.schema.model",
            "model_version": "1",
            "workload_size": "Small"
        }]
    }
)
```

### Lakehouse Monitoring
```python
from databricks import lakehouse_monitoring as lm

lm.create_monitor(
    table_name="catalog.schema.inference_table",
    profile_type=lm.InferenceLog(...),
    schedule=lm.MonitorCronSchedule(...)
)
```

---

## üö® ERRORES COMUNES A EVITAR

‚ùå Usar modelos diferentes para embeddings de docs y queries  
‚ùå No usar overlap en chunking  
‚ùå Ignorar gobernanza (Unity Catalog)  
‚ùå No monitorear en producci√≥n  
‚ùå No tener guardrails  
‚ùå Confiar solo en offline evaluation  
‚ùå No versionar modelos  
‚ùå No usar inference tables  
‚ùå Ignorar costos (pay-per-token)  
‚ùå No recolectar human feedback  

---

## ‚úÖ CHECKLIST ANTES DEL EXAMEN

- [ ] Entiendo la diferencia entre pre-training y fine-tuning
- [ ] S√© qu√© es RAG y c√≥mo funciona
- [ ] Conozco las t√©cnicas de prompt engineering
- [ ] Entiendo chunking strategies
- [ ] S√© qu√© son embeddings y por qu√© el mismo modelo es crucial
- [ ] Conozco Databricks Vector Search
- [ ] Entiendo compound AI systems vs simple systems
- [ ] S√© qu√© es un agent y el patr√≥n ReAct
- [ ] Conozco los frameworks (LangChain, LlamaIndex)
- [ ] Entiendo los m√©todos de deployment (batch, streaming, real-time)
- [ ] Conozco MLflow (Tracking, Registry, Evaluation)
- [ ] S√© qu√© es Model Serving y sus features
- [ ] Entiendo inference tables
- [ ] Conozco Lakehouse Monitoring
- [ ] S√© la diferencia entre MLOps y LLMOps
- [ ] Entiendo guardrails y Llama Guard
- [ ] Conozco las m√©tricas (BLEU, ROUGE, Perplexity)
- [ ] Entiendo LLM-as-a-judge
- [ ] S√© la diferencia entre offline y online evaluation
- [ ] Conozco Unity Catalog y su rol en gobernanza
- [ ] He practicado con los 12 ex√°menes de SkillCertPro

---

## üí° TIPS PARA EL EXAMEN

1. **Lee cuidadosamente**: Muchas preguntas tienen trampa en los detalles
2. **Elimina opciones obviamente incorrectas** primero
3. **Keywords importantes**: "NUNCA", "SIEMPRE", "MEJOR", "√öNICO"
4. **Databricks-specific**: Si hay opci√≥n Databricks vs gen√©rica, probablemente es Databricks
5. **Unity Catalog es clave**: Aparece en muchas preguntas de gobernanza
6. **MLflow est√° en todo**: Tracking, Registry, Evaluation, Deployment
7. **Tiempo**: 90 min para 60 preguntas = 1.5 min/pregunta. Maneja tu tiempo
8. **No te atasques**: Marca y contin√∫a, vuelve despu√©s
9. **Revisa al final**: Si tienes tiempo, revisa respuestas marcadas

---

## üìä DISTRIBUCI√ìN DEL EXAMEN

- 15 preguntas: Fundamentos (25%)
- 18 preguntas: Desarrollo de Soluciones (30%)
- 12 preguntas: Desarrollo de Aplicaciones (20%)
- 9 preguntas: Despliegue y Monitoreo (15%)
- 6 preguntas: Evaluaci√≥n y Gobernanza (10%)

**Total**: 60 preguntas | **Tiempo**: 90 minutos | **Aprobaci√≥n**: 70%

---

## üéì ¬°Buena Suerte!

**√öltimo consejo**: Conf√≠a en tu preparaci√≥n. Si has estudiado las gu√≠as completas y hecho los ex√°menes de pr√°ctica, est√°s listo. üöÄ

---

**Creado para certificaci√≥n**: Databricks Certified Generative AI Engineer Associate

