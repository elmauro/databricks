Generative AI Solution Development

From Prompt Engineering to RAG

	Prompt: input or query to LLM
	
	Prompt Engineering: Prctice of desgining and refining prompts
	
Prompt Components
	Instructions
	Context
	Input/question
	Output type/format
	
	Give some examples of prompts accordring this info
	
	
Prompt Engineering Techniques
	Zero-shot Prompting
		Generate text or perform a task withoit providing any examples or additional training specific to that task
		
	Few-show Prompting
		Provide few input-output examples ti guide the model fir generating the output


Prompting Chaining
	Multiple task are linked and the output of one prompt is the input for the next
	
	This method allows a task to be broken down into manageable steps
	

Chain of Thought Prompting (CoT)
	Enhances the reasoning capabilities of LLMs by guiding them to articulate their process step-by-step like a human reasoning
	

Prompt Engineering Tips and Tricks
	Prompts are model specific
	Different models may require different prompts
	Provide examples a cues to guide models response generation
	Different use cases may require different prompts
	Iterative development is key
	Beaware of bias and hallucination
	

Format Prompts
	Use delimiters to distinguish between instructions and context ###, ```, {}, [], ---
	Ask the model to return structured output HTML, json, table, markdown, etc
	Provide a correct example
	
	
Guide de model for better responses
	Ask the model not to make things up/hallucinate
	Ask the model not to assume or probe for sensitive information
	Ask the model not to rush to a solution
		Ask it to make more time to "think" --> Chain-of-Thought for reasoning
		

Benefits
	Simple and Effcient
	Predictable results
	Tailored outputs
	

Limitations
	The output depends on used model
	Limites by pre-trained models internal models. For external knowledge we need to use a RAG
	

How do Language Models Learn Knowledge
	Model Pre-Training
	Model Fine Tunning
	Passing Contextual Information
	
	
Passing Context to LMs Helps Factual Recall
	
	
What is a RAG
	Retrieval Augmented Generation (Is a pattern that improve the efficacy of LLM by leveraging custom data)
	
	Retrieve data/documents relevant to a question and provide them as context to augment the prompts to an LLM to improve generation
	
	RAG solves a problem of Knowledge GAP enhancing the accuracy and relevance of responses
	

RAG Use Cases
	Q&A Chatbox
	Search Augmentation
	Content Creation and Summarization
	
	What to use for a mechanical car taller?
	
	Does Chat-GPT use RAG?
	
	
Main Concepts of RAG Workflow
	Index & Embed (vector representation of the documents and user queries)
	Vector Store
	Retrieval
	Filtering & Reranking
	
	Prompt Augmentation
	Generation
	
RAG Sample Architecture
	Document Embedding
		Delta Live Tables (batch/stream ingestion)
		Mosaic AI Model Serving (Embeddings)
			Foundational Models
			External Models
		
	Dense Vector Retrieval
		Mosaic AI Model Serving (Embeddings)
		Mosaic AI Vector Search
		
	In-Context Learning
		Unity Catalog's Model Registry
		Mosaic AI Model Serving
			Foundational Models
			Custom Models
	
Benefits of RAG Architecture
	Up-to-date and accurate responses
	Reducing inaccurate responses, or hallucinations
	
	
NOTA: System of Book Title Classfication could be a good option for IA and RAG use


Preparing Data for RAG Solutions

Potential Issues
	Poor quality model output	--> produces incorrect responses
	
	Lost in the middle	--> LLMs tend to overlook the documents
	
	Inefficient retrieval	--> poorly prepared data
	
	Exposing data	--> poor data governance
	
	Wrong embedding model	--> decrease the quality of embeddings and retrieval accuracy


Data prep process
	Ingestion & Pre-processing
	Data Storage & Governance
	Chunking (Fixed size, Context-aware)
	Embedding
	Vector Store
	

Data Storage and Governance


What is Delta Lake?
	Unified data management layer. Is the optimized storage layer that provides the foundation for storing data and tables
	
	
Unity Catalog (UC)
	Governance, discovery, access control for RAG applications
	
	
Data Extraction and Chunking
	Split documents into chunks
	Embed the chunks with a model
	Store them in a vector store
	
	Chunks could be out of context
	No overall logic/rigor
	

How to chunk data
	Title
	Section
	Diagram
	
	Chunk by sentence/paragraph/section
	Leverage special punctuation ('.', '\n')
	include/inject metada/tags/title(s)
	
	Fixed-size Chunking
		Divide by specific number of tokens
		Simple and computationally cheap method
		
		
Chunking Strategy is Use-Case Specific
	Chunking by sentence
	Chunking by paragraph
	
	ChunkViz helper tool
	
	Chunk overlap between consecutive chunks, to not lose contextual information between them
	
	Windowed summarization; each chunk includes a windowed summary of previous few chunks
	

Advanced Chunking Strategies
	Section
		Summarization
			Vector Store
			
			
Data Extraction and Chunking Challenges
	Working with complex documents
		Images
		Text
		Table
		Price and disclamer
		
		Other challenges
			Text mixed with images
			Irregular placement of text
			Color encoded focus
			chart with hierarchical information
			Multi-column text
			Images with related information
			
			
General Approaches
	Traditional Approach
		Libraries 
			PyMuPDF
			PyPDF
			
	Use a Layout Model
		Libraries
			Hugging Face
			doctr
			Donut
			Unstructured
			
	Multi-Modal Models
		Models
			OpenAI
			Alphabet
			Other OSS models
			
			
Embedding Model
	A numerical representation of content
		N-dimensional word vectors/embedding
		

Choosing the right model for your application
	Data/Text properties
		Vocabulary size
		Domain/Topic
		Text Lenght
		
	Model capabilities
		Multi-Language support
		Embedding dimensions/size
		

Practical considerations
	Context window limitations
	Privacy and cost/licensing
	benchmark multiple models and compare
	

Tip1: Choose Your Embedding Model Wisely
		The embeding model should represent BOTH queries and documents
			The embedding model should be trained on similar data
			
Tip2: Ensure similar Embedding Space for both Queries and Documents
		Use the same embedding model or
		Train it with similar data
		
		
Unstructured Data Prep
	Vector Search with Databricks-managed embeddings
	
	External Sources --> Ingestion (tables, volumes) --> Files & Metadata --> Document Processing (Workflows, Delta Live Tables, Notebooks) -->
	Chunks & Features --> Storage (Delta Tables) --> Automatic sync --> Vector DB (Vector Search) 

	Databricks computes the embeddings
		Model Serving
			Custom Models
			External Models (OpenAI, Anthropic, Hugging Face)
			Foundational Models
			

Structure Data Prep
	Feature Serving ad Online Tables
	
	External Sources --> Ingestion (Delta tables) --> Rows --> Feature Engineering (Workflows, Delta Live Tables, Notebooks) --> 
	Features --> Storage (Delta Tables) --> Automatic sync --> Serving (Feature Serving, Online Tables)
	
	

Introduction to Vector Search

What are Vector Database
	Optimized database to store and retrieve high-dimensional vectors such as embeddings
	
	Provide a query interface that retrieves vectors most similar to a specified query vector
	
	
Vector Database has properties like CRUD and spedd up query search for the closest vectors
	Uses vector search algorithms such as approximate nearest neighboo (ANN)
	Organize embeddings into indices
	
Common Use Cases for Vector Databases
	RAG
	Recommendation Engines
	Similarity Search
	

Vector Search Process and Performance

	Distance Metrics (Euclidian Distance, Manhatan Distance)

	Similarity Metrics (Cosine similarity)


Vector Search Strategies

	K-nearest neighbors (KNN)
		ANNOY	--> Spotity
		HNSW	-->	Builds proximity graphs based on Euclidian distance
		FAISS	-->	Facebook
		SCaNN	-->	Google
		
		
Choosing the right vector database

	Pros
		Scalability
		Speed
		Full-fledged Database Properties
			
	Cons
		One more system to learn and integrate
		Added cost
		
		
What about Vector Libraries or Plugins?

	Many don't support filter queries i.e "WHERE"
	
	Libraries create vector indices
		ANN
		Sufficient for small, static data
		NO CRUD support
		Need to wait for full import to finish before queryng
		Store in memory (RAM)
		No data replication
		
	Plugins
		For relational databases like elasticsearch and pgvector
		Less rich features
		Less user-friendly APIs 
		
		
Choosing the Vector Database
	Do you really need it?
	Performance and scalability
	Governance Capabilities
	

How to filter Only Highly Relevant Documents
	Reranking
		Initial Retrieval (use only the relevant documents)
		Reranker (order by scores)
		
		Rerankers
			Private
				Cohere, Jina
				
			Open Source
				Cross-encoders, bge-reranker-base, FlashRank
				
Using Reranking
	Benefits
		Select more relevant documents
		Improve the accuracy of the response
		Reduce hallucinations
		
	Challenges
		The LLM must be called repeatedly. increasing th costs and latency of the RAG chain
		Implement rerankers adds complexity to the RAG pipeline
	


Introduction to Mosaic AI Vector
	Stores vctor representation of your data and metadata
	Integradted with your Lakehouse
	Scalable, low latency production service with zero operational overhead
	Supports ACL using Unity Catalog integration
	API for real-time similarity search (REST and python)
	
How does Vector Search Work?
	Source Delta Table --> Auto Sync	--> 
		Mosaic AI Vector Search Engine
			Indexer
			Vector DB
			Query Engine	<--	REST API Python SDK
				Query as a Text
			
		Mosaic AI Model Serving
			Custom Models
			External Models
			Foundational Models
			
			
	Direct Access CRUD API
		manual sync via API, whit self-managed embeddings
			vsc.similarity_search (query)
			vsc.upsert	(store)
			
Setup Vector Search
	Create a Vector Search Endpoint
	Create a Model Serving Endpoint
	Create a Vector Search Index
	

Introduction to Assembling and Evaluating a RAG Application

RAG Application Workflow

	Data Sources
	
	Development
		Doc/Data pipeline
		Chain
	
	User/Testing
		Stagin deployment to web chat app
	
	Offline Evaluation
		Evaluation harness
	
	Production
		Production deployment
		
	Monitoring and Logging
	

MLflow
	Using MLflow for RAG Solutions
		Open Source for machine learning lifecycle
		Co-developed by Databricks
		Pre-installed on the Databricks runtime for ML
		Operationalizing Generative AI development lifecycle
		
MLflow Model Traking
	Record LLM parameters
	Log metrics to compare performance and accuracy LLMs
	Store and manage output artifacts
	Store model's source code from the run
	
MLflow Model (flavor)
	Each MLflow Model is a directory with a MLmodel file
	MLmodel can define multiple flavors
	MLmodel can contain additional metadata
	
	Model Flavors
		Python Function (mlflow.pyfunc)
		LangChain
		OpenAI
		HuggingFace
		PyTorch
		TensorFlow
		ONNX
		
MLflow Model Registry
	Deploy and organize
	Model versioning
	Model lifecycle management
	Collaboration and permission management
	Full model lineage
	Tagging and annotations
	
	Recommendation: using in Unit Catalog
		
	
Evaluating RAG Pipeline
	Chunking Performance
	Retrieval Performance
	Generator Performance
	
Evaluation Metrics
	Query, Response and Context relations
	
	Context Precision
	
	Context Relevance
		High context relevance
		Low context relevance
	
	Context Recall
		Ground truth
		High-recall context
		Low-recall context
		
	Faithfulness based on Responses and retrieved context
		High Faithfulness answer
		Low Faithfulness answer
		
	Answer Relevancy
		High relevancy answer
		Low relevancy answer
		
		
	Answer Correctness
		High answer correctness
		Low answer correctness
		
	
MLflow (LLM) Evaluation
	Batch comparisson
	Rapid and scalable experimentation
	Cost-Effective
	
	 
Quiero un resumen es español
Quiero que el resumen incluya definiciones del curso de databricks en un lenguaje común y fácil de entender
Quiero que contenga ejemplos simples enfocados en el curso y enfocado en la certificación de databricks
Quiero un resumen actualizado y con definiciones tomadas de los materiales de Databricks – Generative AI Application Evaluation and Governance
Quiero el resumen con un formato más amigable que me permita estudiar y aprender de forma más sencilla
Me gustaría que el resumen resultado sea lo más completo posible con definiciones de databricks y ejemplos 
para poder aprender mejor para la certificación.	
	