# 4ï¸âƒ£ GenAI Application Deployment and Monitoring

## ğŸ“š Table of Contents
1. [GenAI Systems Lifecycle](#genai-systems-lifecycle)
2. [Model Packaging](#genai-model-packaging)
3. [MLflow for Deployment](#mlflow-for-deployment)
4. [Deployment Methods](#deployment-methods)
5. [Databricks Model Serving](#databricks-model-serving)
6. [AI Systems Monitoring](#ai-systems-monitoring)
7. [MLOps and LLMOps](#mlops-and-llmops)

---

## GenAI Systems Lifecycle

### Phase 1: System Development

**Characteristics**: Works with **static data**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. DEFINE PROBLEM                     â”‚
â”‚     What should the system solve?      â”‚
â”‚                                        â”‚
â”‚     Example:                           â”‚
â”‚     "Chatbot that answers questions    â”‚
â”‚      about products"                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. SET METRICS                        â”‚
â”‚     How will success be measured?      â”‚
â”‚                                        â”‚
â”‚     Example:                           â”‚
â”‚     â€¢ Accuracy > 85%                   â”‚
â”‚     â€¢ Latency < 2 seconds              â”‚
â”‚     â€¢ User satisfaction > 4/5          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. COLLECT DATA                       â”‚
â”‚     Obtain relevant data               â”‚
â”‚                                        â”‚
â”‚     Example:                           â”‚
â”‚     â€¢ FAQs                              â”‚
â”‚     â€¢ Support emails                    â”‚
â”‚     â€¢ Product documentation             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. PROCESS DATA                       â”‚
â”‚     Clean and structure                 â”‚
â”‚                                        â”‚
â”‚     â€¢ Remove duplicates                 â”‚
â”‚     â€¢ Normalize text                    â”‚
â”‚     â€¢ Convert formats                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. BUILD SYSTEM                       â”‚
â”‚     Implement the solution              â”‚
â”‚                                        â”‚
â”‚     â€¢ RAG (retrieval + generation)     â”‚
â”‚     â€¢ Chains (structured flows)        â”‚
â”‚     â€¢ Agents (if needed)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. EVALUATE                           â”‚
â”‚     Test against metrics               â”‚
â”‚                                        â”‚
â”‚     â€¢ Manual tests                     â”‚
â”‚     â€¢ Automated metrics                â”‚
â”‚     â€¢ Pilot user feedback              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Phase 2: Deployment & Production

**Characteristics**: **New and changing** data in real time

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. DEPLOYMENT                         â”‚
â”‚     Move to production                 â”‚
â”‚                                        â”‚
â”‚     â€¢ REST API                         â”‚
â”‚     â€¢ Web chatbot                      â”‚
â”‚     â€¢ Mobile app                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. MONITORING                         â”‚
â”‚     Watch behavior                     â”‚
â”‚                                        â”‚
â”‚     â€¢ Detect degradation               â”‚
â”‚     â€¢ Identify errors                  â”‚
â”‚     â€¢ Measure data drift               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. ITERATION (if issues)              â”‚
â”‚                                        â”‚
â”‚     â”Œâ”€â”€> Collect new data              â”‚
â”‚     â”œâ”€â”€> Retrain system                â”‚
â”‚     â”œâ”€â”€> Reprocess data                â”‚
â”‚     â””â”€â”€> Reâ€‘deployment                 â”‚
â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Practical Analogy

**Think of it like developing a mobile app**:

**Phase 1 (Development)**:
- You code the app on your computer
- You test with sample data
- You fix bugs
- It works perfectly in your environment

**Phase 2 (Production)**:
- You publish to the App Store
- Thousands of users download it
- You discover new bugs in real scenarios
- Users use the app in unexpected ways
- You need to update regularly

---

## GenAI Model Packaging

### Ways to Package GenAI Logic

With LLMs, the "ML logic" is packaged in new ways:

#### 1. Engineered Prompt

**What it is**: Carefully designed prompt saved as a template

**Example**:
```python
PROMPT_TEMPLATE = """
You are an expert medical assistant.

Patient context:
{patient_history}

Question:
{question}

Instructions:
- Use precise medical terminology
- Cite sources when possible
- If youâ€™re not sure, say "Consult a doctor"

Answer:
"""
```

**How itâ€™s packaged**:
- Save as `.txt` or `.yaml`
- Version in Git
- Load dynamically at runtime

---

#### 2. Chain (Structured Sequence)

**What it is**: A sequence of steps from frameworks like LangChain or LlamaIndex

**Example**:
```python
from langchain.chains import RetrievalQA
from langchain.vectorstores import FAISS
from langchain.llms import OpenAI

# Define chain
chain = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

# Use
answer = chain.run("What is RAG?")
```

**How itâ€™s packaged**:
- Serialize with `mlflow.langchain.log_model(chain)`
- Includes all dependencies
- Reproducible in any environment

---

#### 3. API Call (Lightweight)

**What it is**: Simple call to an LLM via API

**Types**:

##### a) External Proprietary API
```python
import openai

response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello"}]
)
```

**Advantages**:
- âœ… Easy to use
- âœ… No infrastructure

**Disadvantages**:
- âŒ External dependency
- âŒ Usage cost
- âŒ Data leaves your infrastructure

##### b) Internal API (Selfâ€‘Hosted)
```python
import requests

response = requests.post(
    "https://my-databricks.cloud/serving-endpoints/my-llm",
    json={"prompt": "Hello"}
)
```

**Advantages**:
- âœ… Full control
- âœ… Private data
- âœ… Customizable (fineâ€‘tuned)

**Internal model types**:
- **Fineâ€‘tuned**: Adjusted to your domain
- **Preâ€‘trained**: Unmodified

---

#### 4. Local Invocation with GPU

**What it is**: Run the model locally (not via API)

**Example**:
```python
from transformers import pipeline

generator = pipeline(
    "text-generation",
    model="meta-llama/Llama-2-7b-hf",
    device=0  # GPU 0
)

output = generator("Hello, how are you?")
```

**When to use**:
- Edge deployments (IoT devices)
- No internet connection
- Ultraâ€‘low latency required

**Disadvantages**:
- âŒ Requires GPU/powerful hardware
- âŒ Infrastructure maintenance

---

### Options Summary

| Method | Complexity | Control | Cost | When to Use |
|-------|------------|---------|------|-------------|
| **Prompt** | Low | Low | Variable | Rapid prototype |
| **Chain** | Medium | Medium | Variable | Structured apps |
| **External API** | Low | Low | High | MVP, testing |
| **Internal API** | High | High | Medium | Enterprise production |
| **Local GPU** | High | Total | Low (if HW available) | Edge, offline |

---

## MLflow for Deployment

### What is MLflow?

**Reminder**: Openâ€‘source platform to manage the **full lifecycle** of ML/GenAI

### MLflow Model (Standard Format)

**Structure**:
```
my_model/
â”œâ”€â”€ MLmodel              # Metadata (flavors, signature)
â”œâ”€â”€ conda.yaml           # Dependencies (conda)
â”œâ”€â”€ requirements.txt     # Dependencies (pip)
â”œâ”€â”€ python_env.yaml      # Python version
â”œâ”€â”€ model/               # Model files
â”‚   â”œâ”€â”€ model.pkl
â”‚   â””â”€â”€ tokenizer/
â””â”€â”€ input_example.json   # Input example
```

#### MLmodel File
```yaml
artifact_path: model
flavors:
  langchain:
    langchain_version: 0.0.200
    model_data: model
  python_function:
    env: conda.yaml
    loader_module: mlflow.langchain
    python_version: 3.10.12
signature:
  inputs: '[{"name": "query", "type": "string"}]'
  outputs: '[{"name": "answer", "type": "string"}]'
```

---

### mlflow.pyfunc (Python Function Flavor)

**What it is**: **Generic interface** for any Python model

**Why it matters**:
- Any MLflow model can be loaded as a Python function
- Unified interface for deployment
- Compatible with Model Serving

**Key Functions**:
```python
import mlflow

# Save model
mlflow.pyfunc.log_model(
    artifact_path="model",
    python_model=my_model,
    signature=signature,
    input_example=input_example
)

# Load model
loaded_model = mlflow.pyfunc.load_model("models:/my_model/1")

# Predict
result = loaded_model.predict({"query": "What is RAG?"})
```

---

### MLflow Registry in Unity Catalog

**Features**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  UNITY CATALOG MODEL REGISTRY           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ… Automatic versioning                â”‚
â”‚     â€¢ Version 1, 2, 3, ...             â”‚
â”‚                                         â”‚
â”‚  âœ… Aliases                             â”‚
â”‚     â€¢ @champion (best model)            â”‚
â”‚     â€¢ @challenger (candidate)           â”‚
â”‚     â€¢ @staging                          â”‚
â”‚                                         â”‚
â”‚  âœ… Lifecycle Management                â”‚
â”‚     â€¢ Development â†’ Staging â†’ Prod      â”‚
â”‚                                         â”‚
â”‚  âœ… Collaboration & ACLs                â”‚
â”‚     â€¢ Who can read/write                â”‚
â”‚     â€¢ Unity Catalog governance          â”‚
â”‚                                         â”‚
â”‚  âœ… Full Lineage                        â”‚
â”‚     â€¢ What data was used                â”‚
â”‚     â€¢ What code                         â”‚
â”‚     â€¢ What parameters                   â”‚
â”‚                                         â”‚
â”‚  âœ… Tagging & Annotations               â”‚
â”‚     â€¢ Custom metadata                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Usage Example**:
```python
import mlflow

# Register in Unity Catalog
mlflow.set_registry_uri("databricks-uc")

with mlflow.start_run():
    # Train/build model
    model = build_my_rag_chain()
    
    # Log
    mlflow.langchain.log_model(
        model,
        "model",
        registered_model_name="my_catalog.my_schema.chatbot_v1"
    )

# Set alias
from mlflow import MlflowClient
client = MlflowClient()

client.set_registered_model_alias(
    "my_catalog.my_schema.chatbot_v1",
    "champion",
    version=3
)

# Load model by alias
champion_model = mlflow.langchain.load_model(
    "models:/my_catalog.my_schema.chatbot_v1@champion"
)
```

---

### MLflow and Development Cycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. MODEL/CHAIN BUILDING                â”‚
â”‚     â€¢ Build RAG/Chain/Agent             â”‚
â”‚     â€¢ Experiment with prompts           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. MLFLOW TRACKING                     â”‚
â”‚     â€¢ Log experiments                   â”‚
â”‚     â€¢ Compare metrics                   â”‚
â”‚     â€¢ Select best version               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. MLFLOW EVALUATION                   â”‚
â”‚     â€¢ Evaluate with datasets            â”‚
â”‚     â€¢ Automated metrics                 â”‚
â”‚     â€¢ LLM-as-a-judge                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. MLFLOW REGISTRY                     â”‚
â”‚     â€¢ Register model                    â”‚
â”‚     â€¢ Versioning                        â”‚
â”‚     â€¢ Assign alias (@champion)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. MODEL DEPLOYMENT                    â”‚
â”‚     â€¢ Databricks Model Serving          â”‚
â”‚     â€¢ REST API                          â”‚
â”‚     â€¢ Monitoring                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Deployment Methods

### 1. Batch Inference

**What it is**: Process **large amounts of data at once** on a schedule

**When to use**:
- Daily/weekly reports
- ETL jobs
- No immediate response needed

**Example**:
```
Task: "Summarize all quarterly financial reports"

Execution:
- Every Sunday at 2 AM
- Processes 1000 documents
- Generates summaries
- Saves them to a Delta Table

Users:
- See results Monday morning
```

#### âœ… Advantages
- **Cheaper**: Efficient hardware utilization
- **High volume**: Processes millions of records
- **Efficient**: Parallelizes well

#### âŒ Limitations
- **High latency**: Wait hours/days
- **Stale data**: Not real time
- **Not for streaming**: Data must be static

#### Implementation with Spark
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

# Load data
df = spark.table("my_catalog.my_schema.documents")

# UDF to process with LLM
def summarize_with_llm(text):
    # Call LLM
    return summary

from pyspark.sql.functions import udf
summarize_udf = udf(summarize_with_llm)

# Apply to all
df_summarized = df.withColumn("summary", summarize_udf("text"))

# Save
df_summarized.write.saveAsTable("my_catalog.my_schema.summarized")
```

---

### 2. Streaming Inference

**What it is**: Process **data as it arrives** (stream)

**When to use**:
- Realâ€‘time event processing
- Log analysis
- IoT data

**Example**:
```
Application: "Personalize marketing messages in real time"

Flow:
User clicks â†’ Event captured â†’ 
Stream processor â†’ LLM personalizes message â†’ 
Message sent (< 5 seconds)
```

#### Structured Streaming (Spark)
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

# Read stream from Kafka
stream_df = spark.readStream \
    .format("kafka") \
    .option("subscribe", "user_events") \
    .load()

# Process with LLM
processed_df = stream_df.withColumn(
    "personalized_message",
    personalize_udf("event_data")
)

# Write results
query = processed_df.writeStream \
    .format("delta") \
    .outputMode("append") \
    .option("checkpointLocation", "/checkpoints") \
    .start("/output")
```

---

### 3. Realâ€‘Time Inference

**What it is**: Generate predictions **instantly** when requested

**When to use**:
- Chatbots
- Interactive APIs
- Applications with waiting users

**Example**:
```
User: "Whatâ€™s the status of my order?"
â†“ (< 2 seconds)
API â†’ Model Serving â†’ LLM â†’ Response
â†“
Chatbot: "Your order #12345 is on the way, arriving tomorrow"
```

#### âœ… Advantages
- **Low latency**: Responses in seconds
- **Interactive**: Smooth UX
- **Fresh data**: Uses upâ€‘toâ€‘date information

#### âŒ Challenges
- **Complex infrastructure**: Autoâ€‘scaling, load balancing
- **Cost**: 24/7 servers
- **Requires expertise**: DevOps, SRE

#### Databricks Model Serving
```python
# Already registered in MLflow Registry
```

---

### 4. Embedded/Edge Inference

**What it is**: Run the model **on the device** (no cloud)

**When to use**:
- IoT devices
- Offline mobile apps
- Ultraâ€‘low latency (milliseconds)

**Example**:
```
Autonomous car:
Camera â†’ Local processing (onâ€‘car GPU) â†’ Decision â†’ Action
(Cannot depend on internet)
```

**Example**: Change AC temperature via voice

---

### Method Comparison

| Method | Latency | Cost | Complexity | Best For |
|--------|---------|------|------------|----------|
| **Batch** | High (hours) | Low | Low | Reports, ETL |
| **Streaming** | Medium (seconds) | Medium | Medium | Events, IoT |
| **Realâ€‘Time** | Low (< 2s) | High | High | Chatbots, APIs |
| **Edge** | Very low (ms) | Variable | High | Offline, critical IoT |

---

## ğŸ“¦ APPLICATIONS in Databricks

These three components represent the **production and operations** stage of your models:

- **MLflow AI Gateway**: How they are exposed via API
- **Model Serving**: How they are served efficiently
- **Lakehouse Monitoring**: How they are supervised in real time

---

### ğŸ§© 1. MLflow AI Gateway

The **unified entry point** (API Gateway) to access AI models â€”both internal and externalâ€” securely and in a standardized way.

**What does it do?**:
- âœ… Provides a **single interface** to interact with any LLM:
  - **Internal** (registered in MLflow or Mosaic AI)
  - **External** (OpenAI, Anthropic, Hugging Face, Azure AI, AWS Bedrock, etc.)
- âœ… Controls **costs, authentication, and traceability** of calls
- âœ… Lets teams use LLMs **without exposing individual API keys**

**Problem it solves**:
```
âŒ Without AI Gateway:
Team A â†’ uses OpenAI directly (hardcoded API key)
Team B â†’ uses Anthropic (another key)
Team C â†’ uses internal model (different config)
â†’ Credential chaos
â†’ No centralized tracking
â†’ Uncontrolled costs

âœ… With MLflow AI Gateway:
All teams â†’ AI Gateway â†’ Routes to appropriate model
â†’ Centralized, secure keys
â†’ Automatic logging
â†’ Cost control
```

**Architecture**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  APPLICATION / USER                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MLFLOW AI GATEWAY                       â”‚
â”‚  â€¢ Authentication                        â”‚
â”‚  â€¢ Rate limiting                         â”‚
â”‚  â€¢ Cost tracking                         â”‚
â”‚  â€¢ Logging                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â†“               â†“             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenAI   â”‚  â”‚ Anthropicâ”‚  â”‚ DBRX     â”‚
â”‚ GPTâ€‘4    â”‚  â”‚ Claude   â”‚  â”‚ Internal â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Usage Example**:
```python
from mlflow.deployments import get_deploy_client

# Gateway client
client = get_deploy_client("databricks")

# Call GPTâ€‘4 (without exposing API key)
response = client.predict(
    endpoint="chat-gpt-4",
    inputs={
        "messages": [
            {"role": "user", "content": "Explain RAG"}
        ]
    }
)

# Call Claude (same interface)
response_claude = client.predict(
    endpoint="chat-claude",
    inputs={
        "messages": [
            {"role": "user", "content": "Explain RAG"}
        ]
    }
)

# All calls are logged automatically
```

**Advantages**:
- ğŸ” **Security**: Centralized keys
- ğŸ’° **Cost control**: Unified tracking
- ğŸ“Š **Observability**: Automatic logging
- ğŸ”„ **Flexibility**: Switch providers without changing code

---

### âš™ï¸ 2. Databricks Model Serving

Service that **deploys models** (ML or LLMs) as **REST endpoints in production**, optimized for low latency and high availability.

**Supported Model Types**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DATABRICKS MODEL SERVING               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  1. CUSTOM MODELS (MLflow)              â”‚
â”‚     â€¢ Your model registered in UC       â”‚
â”‚     â€¢ Custom chains, RAGs, Agents       â”‚
â”‚     â€¢ Python, PyTorch, TensorFlow       â”‚
â”‚                                         â”‚
â”‚  2. FOUNDATION MODELS                   â”‚
â”‚     â€¢ DBRX Instruct                     â”‚
â”‚     â€¢ LLaMA 3 (8B, 70B)                 â”‚
â”‚     â€¢ Mixtral 8x7B                      â”‚
â”‚     â€¢ BGEâ€‘Large (embeddings)            â”‚
â”‚                                         â”‚
â”‚  3. EXTERNAL MODELS                     â”‚
â”‚     â€¢ OpenAI (GPTâ€‘4, GPTâ€‘3.5)           â”‚
â”‚     â€¢ Anthropic (Claude)                â”‚
â”‚     â€¢ Cohere                            â”‚
â”‚     â€¢ AWS Bedrock models                â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Features**:

| Function | Description | Advantage |
|----------|-------------|-----------|
| **Real Time** | Instant responses | Chatbots, interactive APIs |
| **Batch Inference** | Massive processing | Periodic reports |
| **MLflow Integration** | Deploy directly from Registry | No manual config |
| **Security UC** | Access control via Unity Catalog | Enterprise governance |
| **Inference Tables** | Save every prediction | Audit and debugging |
| **Autoâ€‘scaling** | Scales by demand | Cost optimization |
| **Scaleâ€‘toâ€‘zero** | Turns off when idle | Saves costs |

---

### Inference Tables

**What they are**: Each requestâ€‘response is automatically saved in a **Delta Table**

**What theyâ€™re for**:
- âœ… Post monitoring
- âœ… Debugging
- âœ… Usage analysis
- âœ… Reâ€‘training (feedback loop)

**Structure**:
```
Inference Table:
â”œâ”€â”€ request_id
â”œâ”€â”€ timestamp
â”œâ”€â”€ input (user query)
â”œâ”€â”€ output (LLM response)
â”œâ”€â”€ latency
â”œâ”€â”€ token_count
â””â”€â”€ metadata
```

**Enable it**:
```python
w.serving_endpoints.create(
    name="my-endpoint",
    config={
        "served_models": [{ ... }],
        "traffic_config": { ... },
        "auto_capture_config": {
            "catalog_name": "my_catalog",
            "schema_name": "my_schema",
            "table_name_prefix": "inference_"
        }
    }
)
```

**Result**: Itâ€™s automatically created:
```
my_catalog.my_schema.inference_my-endpoint
```

---

### A/B Testing and Canary Deployments

#### Traffic Splitting

**Example**: Test a new model with 10% of traffic
```python
w.serving_endpoints.update_config(
    name="my-endpoint",
    config={
        "served_models": [
            {
                "model_name": "my_catalog.my_schema.chatbot",
                "model_version": "3",  # Old version
                "workload_size": "Small"
            },
            {
                "model_name": "my_catalog.my_schema.chatbot",
                "model_version": "4",  # New version
                "workload_size": "Small"
            }
        ],
        "traffic_config": {
            "routes": [
                {"served_model_name": "chatbot-3", "traffic_percentage": 90},
                {"served_model_name": "chatbot-4", "traffic_percentage": 10}
            ]
        }
    }
)
```

**Flow**:
```
100 requests
â”œâ”€> 90 requests â†’ Model v3
â””â”€> 10 requests â†’ Model v4

Analyze v4 metrics:
- If better: gradually increase %
- If worse: revert to 100% v3
```

---

### Unity Catalog Integration
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  UNITY CATALOG                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  â€¢ UC Volume                            â”‚
â”‚    â””â”€> Files (PDFs, CSVs, etc.)         â”‚
â”‚                                         â”‚
â”‚  â€¢ Raw/Processed Text Tables            â”‚
â”‚    â””â”€> Processed documents              â”‚
â”‚                                         â”‚
â”‚  â€¢ Embeddings/Index                     â”‚
â”‚    â””â”€> Vector Search index              â”‚
â”‚                                         â”‚
â”‚  â€¢ Model/Chain                          â”‚
â”‚    â””â”€> Registered model                 â”‚
â”‚                                         â”‚
â”‚  â€¢ Inference Table                      â”‚
â”‚    â””â”€> Request logs                     â”‚
â”‚                                         â”‚
â”‚  â€¢ Processed Payloads Table             â”‚
â”‚    â””â”€> Processed data                   â”‚
â”‚                                         â”‚
â”‚  â€¢ Metric Tables                        â”‚
â”‚    â””â”€> Lakehouse Monitoring             â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Everything in Unity Catalog** = Centralized governance

---

## AI Systems Monitoring

### Why Monitor?

**Goal**: Diagnose problems **before** they become severe or costly

**Without monitoring**:
```
User: "The chatbot gives weird answers"
â†’ When did it start?
â†’ How many users affected?
â†’ Which inputs cause the issue?
â†’ â“ We donâ€™t know
```

**With monitoring**:
```
Automatic alert: "30% increase in lowâ€‘quality answers since 2 PM"
â†’ We know exactly when and how much
â†’ We see problematic inputs
â†’ We can act quickly
```

---

### What to Monitor?

#### 1. Input Data
- Query distribution
- Input length
- Languages used
- Detection of malicious inputs (prompt injection)

#### 2. Data in Vector Databases
- Document quality
- Topic coverage
- Information freshness

#### 3. Human Feedback
- ğŸ‘ğŸ‘ reactions
- User comments
- Ratings (1â€“5 stars)

#### 4. Prompts/Queries and Responses
- Store for analysis
- âš ï¸ **Legality**: Can you store user data?
- Privacy compliance (GDPR, CCPA)

---

### AI Assets to Monitor

#### 1. Midâ€‘Training Checkpoints
- If fineâ€‘tuning, periodic checkpoints
- Loss curve analysis

#### 2. Component Evaluation Metrics
- Retrieval quality
- Embedding quality
- Reranking effectiveness

#### 3. System Evaluation Metrics
- Endâ€‘toâ€‘end latency
- Token usage (cost)
- Answer quality

#### 4. Performance & Cost
- Requests per second (RPS)
- P50, P95, P99 latency
- GPU utilization
- Cost per request

---

### ğŸ“Š 3. Lakehouse Monitoring

A **fully managed** solution to monitor behavior, performance, and quality of **production models** (ML and GenAI).

**What does it monitor?**:
- âœ… **Input and output data** â†’ Detect data drift or quality degradation
- âœ… **Model performance** â†’ Latency, costs, errors
- âœ… **Custom metrics** â†’ Accuracy, faithfulness, relevance
- âœ… **Trends over time** â†’ Automatic dashboards in Databricks SQL

**Internal Architecture**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INFERENCE TABLE (Delta Table)           â”‚
â”‚  â€¢ request_id                            â”‚
â”‚  â€¢ timestamp                             â”‚
â”‚  â€¢ input (query)                         â”‚
â”‚  â€¢ output (response)                     â”‚
â”‚  â€¢ latency, tokens, metadata             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAKEHOUSE MONITORING                    â”‚
â”‚  â€¢ Periodically analyzes                 â”‚
â”‚  â€¢ Generates automatic metrics           â”‚
â”‚  â€¢ Detects drift, anomalies              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â†“               â†“             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Profile  â”‚  â”‚  Drift   â”‚  â”‚ Custom   â”‚
â”‚ Metrics  â”‚  â”‚ Metrics  â”‚  â”‚ Metrics  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚               â”‚             â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DATABRICKS SQL DASHBOARD                â”‚
â”‚  â€¢ Automatic visualizations              â”‚
â”‚  â€¢ Configurable alerts                   â”‚
â”‚  â€¢ Historical metrics                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Features**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAKEHOUSE MONITORING                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ… Fully Managed                       â”‚
â”‚     â€¢ Zero ops overhead                 â”‚
â”‚     â€¢ No infrastructure to maintain     â”‚
â”‚                                         â”‚
â”‚  âœ… Frictionless                        â”‚
â”‚     â€¢ Setup in minutes                  â”‚
â”‚     â€¢ Simple configuration              â”‚
â”‚                                         â”‚
â”‚  âœ… Unified                             â”‚
â”‚     â€¢ Data + Models in one place        â”‚
â”‚     â€¢ All in Unity Catalog              â”‚
â”‚                                         â”‚
â”‚  âœ… Built on Unity Catalog              â”‚
â”‚     â€¢ Integrated ACLs                   â”‚
â”‚     â€¢ Automatic governance              â”‚
â”‚                                         â”‚
â”‚  ğŸ“Š Autoâ€‘generates DBSQL Dashboard      â”‚
â”‚     â€¢ Ready visualizations              â”‚
â”‚     â€¢ Interactive charts                â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Metric Types

**1. Profile Metrics**:
- Data statistics (min, max, mean, std)
- Distributions
- Nulls, missings

**2. Drift Metrics**:
- Did data change vs baseline?
- Distributional drift
- Statistical tests (KS, Ï‡Â²)

**3. Custom Metrics**:
- Define your own
- Example: "% of answers > 500 words"

---

#### Lakehouse Monitoring Setup
```python
from databricks import lakehouse_monitoring as lm

# Monitor inference table
lm.create_monitor(
    table_name="my_catalog.my_schema.inference_chatbot",
    profile_type=lm.InferenceLog(
        timestamp_col="timestamp",
        model_id_col="model_version",
        prediction_col="output",
        problem_type="text-generation",
        label_col=None  # No ground truth in real time
    ),
    output_schema_name="my_catalog.my_schema",
    schedule=lm.MonitorCronSchedule(
        quartz_cron_expression="0 0 * * * ?"  # Every hour
    )
)
```

**Automatic result**:
1. **Profile Table**: Statistics per time window
2. **Drift Table**: Drift metrics
3. **Dashboard**: Visualization in Databricks SQL

---

### Monitoring Workflow
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DEVELOPMENT                            â”‚
â”‚  â€¢ Create monitoring tables for all     â”‚
â”‚    components                           â”‚
â”‚  â€¢ Define key metrics                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TESTING                                â”‚
â”‚  â€¢ Validate monitoring works            â”‚
â”‚  â€¢ Adjust alert thresholds              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PRODUCTION                             â”‚
â”‚  â€¢ Regular table refresh                â”‚
â”‚  â€¢ Alerts configured                    â”‚
â”‚  â€¢ Dashboards monitored                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ACTION                                  â”‚
â”‚  â€¢ If issues:                            â”‚
â”‚    - Investigate root cause              â”‚
â”‚    - Collect new data                    â”‚
â”‚    - Retrain/adjust                      â”‚
â”‚    - Reâ€‘deployment                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tips**:
- âœ… **Model cost vs performance**: Is a more expensive model worth it?
- âœ… **Regular refresh**: Hourly, daily (based on criticality)
- âœ… **Key alerts**: Not all metrics, only critical ones
- âœ… **Retrain triggers**: Automate if drift > threshold

---

## MLOps and LLMOps

### What is MLOps?

**Definition**: Set of **processes and automation** to manage data, code, and models, improving performance, stability, and efficiency of ML systems.

**Formula**:
```
MLOps = DataOps + DevOps + ModelOps
```

**Components**:
- **DataOps**: Data management (quality, pipelines, versioning)
- **DevOps**: CI/CD, infrastructure as code, deployment
- **ModelOps**: Model management (training, evaluation, deployment, monitoring)

---

### Why does MLOps matter?

| Benefit | Description |
|---------|-------------|
| **Data quality** | Clean, trustworthy data |
| **Optimized processes** | Less manual work, more automation |
| **Cost/performance monitoring** | Optimize ROI |
| **Time to value** | Faster to production |
| **Less manual oversight** | Automate checks |

---

### Multiâ€‘Environment Semantics

**Standard environments**:
```
1. DEVELOPMENT (Dev)
   â€¢ Experimentation
   â€¢ Breaking changes allowed
   â€¢ Synthetic/sampled data

2. STAGING (Stage)
   â€¢ Preâ€‘production
   â€¢ Rigorous testing
   â€¢ Data similar to prod

3. PRODUCTION (Prod)
   â€¢ Real users
   â€¢ High availability
   â€¢ 24/7 monitoring
```

---

### Environment Separation

#### Option 1: Direct Separation

**Strategy**: **Completely separate Databricks workspaces**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dev Workspace   â”‚  â”‚ Stage Workspace  â”‚  â”‚  Prod Workspace  â”‚
â”‚                  â”‚  â”‚                  â”‚  â”‚                  â”‚
â”‚  â€¢ Experiments   â”‚  â”‚  â€¢ Preâ€‘prod      â”‚  â”‚  â€¢ Users         â”‚
â”‚  â€¢ Sample data   â”‚  â”‚  â€¢ Testing       â”‚  â”‚  â€¢ High avail.   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros**:
- âœ… Complete isolation
- âœ… No risk to affect prod
- âœ… Scales to multiple projects

**Cons**:
- âŒ More infrastructure
- âŒ More permission complexity

---

#### Option 2: Indirect Separation

**Strategy**: **Single workspace with logical separation** (catalogs, schemas, prefixes)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Databricks Workspace                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Catalog: dev_catalog                   â”‚
â”‚    â””â”€> Schema: chatbot                  â”‚
â”‚                                         â”‚
â”‚  Catalog: staging_catalog               â”‚
â”‚    â””â”€> Schema: chatbot                  â”‚
â”‚                                         â”‚
â”‚  Catalog: prod_catalog                  â”‚
â”‚    â””â”€> Schema: chatbot                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros**:
- âœ… Less infrastructure
- âœ… Simpler permissions

**Cons**:
- âŒ Risk of mixing environments
- âŒ Doesnâ€™t scale well to many projects

---

### Recommended Architecture (MLOps)

**Deployâ€‘Code Architecture**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SOURCE CONTROL (Git)                      â”‚
â”‚  â€¢ Code                                    â”‚
â”‚  â€¢ Configurations                          â”‚
â”‚  â€¢ Infrastructure as Code                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”œâ”€â”€â”€â”€â”€> Dev Workspace
              â”œâ”€â”€â”€â”€â”€> Staging Workspace
              â””â”€â”€â”€â”€â”€> Prod Workspace
              
CI/CD Pipeline:
  Git Push â†’ Tests â†’ Build â†’ 
  Deploy to Dev â†’ Tests â†’ 
  Deploy to Staging â†’ Tests â†’ 
  Manual Approval â†’ Deploy to Prod
```

---

### LLMOps: Differences vs Traditional MLOps

| Aspect | Traditional MLOps | LLMOps |
|--------|-------------------|--------|
| **Dev Patterns** | Code + data | **+ Text templates (prompts)** |
| **Packaging** | Serialized model | **Complete applications (chains)** |
| **Serving** | Model endpoint | **+ Vector DB, GPU infra, UI** |
| **API Governance** | Less critical | **Critical** (endpoint access) |
| **Cost** | Fixed (infra) | **Variable** (usage, API calls) |
| **Human Feedback** | Optional | **Essential** (improve prompts) |

---

### LLMOps: Specific Aspects

#### 1. Incremental Development
- Iterate prompts
- Adjust chains without retraining

#### 2. Text Templates
- Version prompts as code
- A/B testing of prompts

#### 3. Entire Applications
- Not just the model, the whole app (retriever + LLM + UI)

#### 4. Additional Components
- Vector databases
- Embedding services
- Rerankers

#### 5. GPU Infrastructure
- Requires powerful GPUs
- Optimize batch size, quantization

#### 6. Cost Management
- APIâ€‘based models: payâ€‘perâ€‘token
- Techniques to reduce cost:
  - Use smaller models when possible
  - Cache common responses
  - Prompt compression

#### 7. Human Feedback Loop
- Collect feedback (thumbs up/down)
- Use it to improve prompts
- Fineâ€‘tuning with feedback

---

### Recommended LLMOps Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DEVELOPMENT                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Experiment with prompts                 â”‚
â”‚  â€¢ Build chains/RAGs                       â”‚
â”‚  â€¢ Unit test components                    â”‚
â”‚  â€¢ MLflow tracking                         â”‚
â”‚                                            â”‚
â”‚  Dev Workspace                             â”‚
â”‚    dev_catalog.chatbot.models              â”‚
â”‚    dev_vector_search_index                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGING                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Integration testing                     â”‚
â”‚  â€¢ Performance testing                     â”‚
â”‚  â€¢ Canary deployment                       â”‚
â”‚  â€¢ Evaluation with test set                â”‚
â”‚                                            â”‚
â”‚  Staging Workspace                         â”‚
â”‚    staging_catalog.chatbot.models          â”‚
â”‚    staging_vector_search_index             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PRODUCTION                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Model Serving (autoâ€‘scaling)            â”‚
â”‚  â€¢ Lakehouse Monitoring                    â”‚
â”‚  â€¢ Human feedback collection               â”‚
â”‚  â€¢ Inference tables logging                â”‚
â”‚                                            â”‚
â”‚  Prod Workspace                            â”‚
â”‚    prod_catalog.chatbot.models@champion    â”‚
â”‚    prod_vector_search_index                â”‚
â”‚    prod_inference_logs                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Practice Questions

### Question 1
**Which deployment method is best for an interactive chatbot?**

A) Batch  
B) Streaming  

### Question 2
**What do Inference Tables automatically store?**

A) Only inputs  
B) Only outputs  
C) Full requests and responses âœ…  
D) Only aggregated metrics

**Answer**: C â€“ Each requestâ€‘response is logged for monitoring

---

### Question 3
**In A/B testing, what percentage would you initially give a new model?**

A) 50%  
B) 90%  
C) 10â€“20% âœ…  
D) 100%

**Answer**: C â€“ Start with low traffic to minimize risk

---

### Question 4
**Which MLflow component manages model versions and aliases?**

A) MLflow Tracking  
B) MLflow Registry âœ…  
C) MLflow Evaluation  
D) MLflow Projects

**Answer**: B â€“ Registry = versioning, aliases, lifecycle

---

### Question 5
**What differentiates LLMOps from traditional MLOps?**

A) LLMOps uses Python  
B) LLMOps includes text templates and human feedback âœ…  
C) LLMOps is simpler  
D) No difference

**Answer**: B â€“ LLMOps adds prompts, chains, feedback loop

---

## ğŸ“ Executive Summary

### What you MUST know:

âœ… **Lifecycle**: Development (static data) â†’ Production (new data)  
âœ… **Packaging**: Prompt, Chain, API call (external/internal), Local GPU  
âœ… **MLflow pyfunc**: Generic interface for deployment  
âœ… **Unity Catalog Registry**: Versioning, aliases (@champion), lineage, ACLs  
âœ… **Deployment Methods**:
   - Batch = high volume, low urgency
   - Streaming = realâ€‘time events
   - Realâ€‘Time = chatbots, APIs (< 2s)
   - Edge = offline, IoT

âœ… **Model Serving**: Custom, Foundation, External models  
âœ… **Inference Tables**: Autoâ€‘logging of requests for monitoring  
âœ… **A/B Testing**: Traffic splitting to test versions  
âœ… **Lakehouse Monitoring**: Profile, Drift, Custom metrics  
âœ… **MLOps** = DataOps + DevOps + ModelOps  
âœ… **LLMOps** = MLOps + prompts + chains + human feedback + cost management

---

## ğŸ”— Next Topic

â¡ï¸ **Continue with**: `05_Evaluation_Governance.md` (Evaluation, Security, Guardrails, Metrics)
