Generative AI Application Deployment and Monitoring

Model Deployment Fundamentals

The Gen AI System Lifecycle


Fase 1: System Development (usa datos estáticos)

Un dataset para entrenamiento y prueba

1. 	Se define el problema que el sistema de IA debe resolver.

	Ejemplos:
		- Un Agente que pueda dr información de sitios turísticos
		- Un Agente que pueda dar información de primera infancia
		
		
2.	Se establecen métricas claras para saber si el sistema funciona correctamente.

	Ejemplo: Precisión del 85%, tiempos de respuesta < 2 segundos.
	
	
3.	Se recopilan datos relevantes para el problema.

	Ejemplo: Preguntas frecuentes, correos de soporte, documentos técnicos.
	
	
4.	Limpieza, transformación y estructuración de los datos.

	Ejemplo: Quitar duplicados, normalizar texto, convertir formatos.
	
	
5.	Se construye el sistema de IA usando técnicas como:

	RAG (Retrieval-Augmented Generation): Combina búsqueda + generación.

	Chains: Flujo estructurado de pasos de IA (como en LangChain).
	
	
6.	Se evalúa el sistema usando los criterios definidos.

	Puede involucrar pruebas manuales, métricas automatizadas, etc.
	
	
Fase 2: Deployment & Production (datos nuevos y cambiantes)

Una vez que el sistema funciona, se despliega y se enfrenta a nuevos datos en tiempo real.

1.	El modelo es llevado a producción (por ejemplo, en una API, chatbot, app, etc.).

2.	Se monitorea el comportamiento del sistema en producción.

	Detecta degradación del rendimiento, errores, "drift" de datos.

	Si hay problemas, se puede:

		Recolectar nuevos datos

		Reentrenar el sistema

		Reprocesar datos
		
Si el rendimiento baja, puedes recolectar más datos, hacer ajustes al modelo o redefinir criterios.


GenAI Deployment: Empaquetando modelos/pipelines

Con LLMs (Modelos de Lenguaje Grandes), la lógica de ML se empaqueta en nuevas formas, como por ejemplo:

	Engineered prompt
	Un prompt diseñado a mano que puede guardarse como plantilla.

	Ejemplo: "Eres un asistente médico. Resume este historial clínico:".
	
	
	“Chain” desde LangChain, LlamaIndex, etc.
	Secuencia estructurada de pasos que organiza cómo se hacen llamadas al LLM.

	Ejemplo: búsqueda → extracción de contexto → generación de respuesta.
	
	
	Llamada ligera a un servicio API LLM
	Se llama a un modelo usando una API.

	Puede ser:

	Internamente hospedado (por ti mismo, como un modelo en Databricks o tu propio servidor con DBRX).

	Proveedor externo propietario como OpenAI, Anthropic, Cohere, etc.
	
	
	Llamada ligera a una API LLM propia (bespoke/self-hosted)
	Cuando tú mismo creaste y alojas tu modelo (fine-tuned o preentrenado).

	Puede ser:

	Modelo fine-tuned: adaptado a tu dominio (ej. legal, médico).

	Modelo preentrenado: sin modificar, tal como viene de la fuente.
	
	
	Invocación local con GPU
	Ejecutas el modelo localmente (no vía API).

	Usas pipelines como Hugging Face + tokenizer corriendo en GPUs.

	Útil para despliegues en edge, sin depender de la nube.
	
	
aunque la forma de desplegar cambia (prompt, chain, API, local), todo sigue siendo una "pipeline" o "modelo" desde la perspectiva de MLOps o IA.



¿Qué es MLflow?
MLflow es una plataforma open-source que gestiona el ciclo de vida completo de un modelo de machine learning o GenAI.

Model Management with MLFlow

	Manage end-to-end ML and GenAI workflows, from development to production

	traditionalML and GenAI applications
	
	Generative AI specific model flavors and evalutions metrics
	
	
MLFlow - Model ("flavor")
	standar format for packaging machine learning models
	
	Each MLflow Model is a directory with files and an MLmodel file
	
	MLModel file can define multiple flavors that the model can be viewed in
	
	The deployment tools can understand the model
	
	Model file can contain additional metadata such as signature, input example etc.
	
	mlflow.pyfunc
		Serves a defult model interface for MLFlow Python models
		
		Any MLflow Python model is expected to be loadable as python function
		
		Allows you to deploy models as Python functions
		
		some functions: log_model, save_model, load_model, predict
		
		
MLFlow and Development lifecycle
	Model/Chain Building
	
	mlflow Tracking and Evaluation
	
	mlflow Registry
	
	Model Deployment
	
	
MLFlow - Unity Catalog Model Registry
	Model lifecycle management with versioning and @aliases
	
	Deploy and organize models
	
	Collaboration and ACLs
	
	Full mode lineage
	
	Tagging and annotations
	
	
MLFlow and GenAI Development
	Dependency and Environment Management
	
	Packaging Models and Code
	
	Multiple Deployment Options
	
	
	
Deployments Methods
	batch
		Summarizing financial reports and generating insights
		
		On a regular schedule and writes the results out to persisten storage
		
		Is the simplest deployment strategy
		
		Advantages:
			Cheapset
			Ease
			Efficient
			High volume of data
			
		Limitations:
			High latency
			Stale data
			Not for dynamically or rapidly changing data
			Not for streaming data or real-time applications
			
		Access to GPUs with large memory
		Budget: cost of acquiring/provisioning HW while ensuring maximum utilization
		Parallelization is not trivial
		
		Other btach inference methods:
			TenorRT
			
			vLLM
			
			Ray on spark
		
		
	streaming
		Personalizing maketings messages
		
	real-time
		Chatbots (customer services, doc assitant)
		
		Serving machine learnng models in a production environment where predictions are generated instantly

		For applications that require low-latency responses, such chatbots, messages intent detection, autonomous systems
		
		Challenge:
			Infrastructure is hard, scalable, costly
			
			Requires disparate tools
			
			Requires expert resources
			
		Production-Grade Serving
		Accelerate deployments with Lakehousing-Unified Serving
		Simplified Deployment
		
	embedded/edge
		Modify air conditioning temperature in a car using voice command
		

Databricks Model Serving
	RAG/Chain Building
	
	Tracking and Evaluation
	
	mlflow Registry
	
	Databricks Model Serving
		Custom Models
			MLFlow
		
		Foundation Models
			Databricks
		
		External Models
			OpenAI, etc
			
		Deploy registered models in real-time using production-grade APIs
		
		Inferece tables
			for monitoring ans debugging models. Each [request-response] is append to a delta table in Unity Catalog
			
			Unity Catalog
				UC Volume
				Raw/Processed text tables
				Embeddings/Index
				Model/Chain
				Inferece table
				
	Online Evaluation
		Strategies such as A/B testing or canary deployments
		

Monitoring AI Systems
	Why
		Used to help diagnose issues before they become severe or costly
	
	Data to Monitor
		Input Data
		Data in vector databases/knowledge bases
		Human feedback data
		prompt/queries and responses (legality)
		
	AI Assets to Monitor
		Mid-training ehckpoints for analysis
		Component evaluation metrics
		AI System evaluation metrics
		Performance/cost details
		
	Dataricks Lakehouse Monitoring
		Fully managed
		Frictionless
		Unified
		
	Buil on Unity Catalog
		profile metrics
		drif metrics
		custom metrics
		Auto-generates DBSQL dashboard
		
	Monitoring model's responses
		Monitoring an inference table
			Unity Catalog
				UC Volume
				Raw/Processed text tables
				Embeddings/Index
				Model/Chain
				Inferece table
				Processed payloads table
				Metric tables
				
	Lakehouse Monitoring Workflows
		Architecure Stages
			Development
			Testing
			Production
			
		Workflow Tips
			Setup monitoring tables for all components
			Model cost agains performance
			Refresh the tables and dashboard regularly
			Set up key monitoring alerts
			Connect return triggers to performance
			
			
MLops Primer
	What is MLOps
		Is the set of processes and automation
		for managing data, code and models
		to improve performance, stability and long-term efficiency of ML Systems
		
		MLops = DataOps + DevOps + ModelOps
		
		Why matter?
			Quality data
			Streamlining process
			Cost y performance monitoring
			Acelerated time to realizing bussines value
			Reduction in manual oversight
			
		Multi-environment Semantics
			Development
			Staging
			Production
			
		Environment Separation
		
			Direct Separation
				Complete separate Databricks workspaces for each environment
				Simpler environments
				Scales well to multiple projects
				
			Indirect Separation
				One Databricks workspace with enforced separation
				Simpler overall infrastructure requiring less permission required
				Complex individual environment
				Doesn't scale well to multiple projects
				
		Recommended "deploy-code" architecture (MLOps)
			
		A Single Platform for Modern MLOps
			Combining DataOps, DevOps, and ModelOps solutions
				Lakehouse Data Architecture and Storage
				
		Recommended LLMOps Architecture
			Development
			Staging
			Production
			
			
What about LLMOps?
	Dev Patterns
		Incremental
		Inclusion of text templates
	
	Packaging
		Entire applications
		Environment configurations
	
	Serving
		Additional components like vector databases
		GPU infrastructure
		User interface components
	
	API Governance
		Managing access to endpoints
		Governing us if applications
	
	Cost and Performance
		Model size increases cost
		(API-based) Models hace use-based cost
		Techniques for reducing model size/cost
	
	Human Feedback
		Collection of data
		Use of data
	

Quiero un resumen es español
Quiero que el resumen incluya definiciones del curso de databricks en un lenguaje común y fácil de entender
Quiero que contenga ejemplos simples enfocados en el curso y enfocado en la certificación de databricks
Quiero un resumen actualizado y con definiciones tomadas de los materiales de Databricks – Generative AI Application Deployment and Monitoring
Quiero el resumen con un formato más amigable que me permita estudiar y aprender de forma más sencilla
Me gustaría que el resumen resultado sea lo más completo posible con definiciones de databricks y ejemplos 
para poder aprender mejor para la certificación.